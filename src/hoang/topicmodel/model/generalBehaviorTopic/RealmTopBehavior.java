package hoang.topicmodel.model.generalBehaviorTopic;

import hoang.topicmodel.data.Behavior;
import hoang.topicmodel.data.User;
import hoang.topicmodel.data.Tweet;
import hoang.tool.*;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.util.HashMap;
import java.util.Random;

import org.apache.commons.io.FilenameUtils;


public class RealmTopBehavior {
	//
	public String dataPath;
	public String outputPath;
	public String learntModelPath;
	public int nRealms;
	public int nTopics;
	public Random rand;

	// hyperparameters
	private double alpha;
	private double sum_alpha;
	private double tau;
	private double sum_tau;
	private double rho;
	private double sum_rho;
	private double eta;
	private double sum_eta;
	private double beta;
	private double sum_beta;
	private double gamma;
	private double[] sum_gamma;

	// data
	private User[] users;
	private String[] tweetVocabulary;
	private String[][] behaviorVocabularies;

	// parameters
	private double[][] realmTopicDistribution;
	private double[][] tweetTopics;
	private double[][][] behaviorTopics;

	// Gibbs sampling variables
	// user - coin count
	private int[][] n_cu; // n_z[v,u]: number of times coin v is observed in
							// tweets and behaviors by user u
	private int[] sum_ncu;// sum_nz[u]: total number of coins that are observed
							// in tweets and behaviors by user u
	// user - realm count
	private int[][] n_ru; // n_gu[r,u]: number of times realm r is
							// observed in tweets and behaviors by user u
	private int[] sum_nru;// sum_ngu[u]: total number of realms that
							// are observed in tweets and behaviors by user
							// u
	// user - topic count
	private int[][] n_zu; // n_zu[k,u]: number of times topic z is observed in
							// tweets by user u
	private int[] sum_nzu;// sum_nzu[u]: total number of topics that are
							// observed in tweets by user u
	// realm - topic count
	private int[][] n_zr; // n_zr[k,r]: number of times topic z is observed in
							// realm r
	private int[][] n_rz; // n_zg[r,k]: number of times topic z is observed in
							// realm r
	private int[] sum_nzr;// sum_nzg[r]: total number of topics that are
							// observed in realm r
	private int[] sum_nrz;// sum_nzg[z]: total number of realms that are
							// observed in topic z
	// topic - word count
	private int[][] n_wz;// n_w[w,z]: number of times word w is generated by a
							// topic z in all tweets
	private int[] sum_nwz;// sum_nw[z]: total number of words that are generated
							// by a topic z in tweets
	// behaviors
	private int[][][] n_lbz;// n_w[l,b,z]: number of times a the behavior b of
							// type l is generated by topic z
	private int[][] sum_nlbz;// n[l,z]: total number of behaviors of type l that
								// are generated by topic z

	// users: topic and coin
	private int[][][] n_uzc; // n_uzv[u][z][c]: number of time topic z is
								// associated with coin c in bag of tweets and
								// behaviors of u
	private int[][] n_zc; // n_zv[z][c]: number of time topic z is
							// associated with coin c

	private int[][] final_n_cu;
	private int[] final_sum_ncu;
	private int[][] final_n_ru;
	private int[] final_sum_nru;
	private int[][] final_n_zu;
	private int[] final_sum_nzu;
	private int[][] final_n_zr;
	private int[] final_sum_nzr;
	private int[][] final_n_wz;
	private int[] final_sum_nwz;
	private int[][][] final_n_lbz;
	private int[][] final_sum_nlbz;
	private int[][] final_n_zc;

	private int nBehaviorTypes;
	private String[] behaviorNames;
	private String[] behaviorFolders;

	private double tweetLogLikelidhood;
	private double tweetLogPerplexity;
	private double[] behaviorLogLikelidhoods;
	private double[] behaviorLogPerplexities;

	// learning options
	public boolean topicCoinSparityFlag;
	public boolean topicRealmSparityFlag;
	public boolean globalOrPersonalTopicCoinSparityFlag;
	public boolean realmTopicSparityFlag;

	public int burningPeriod;
	public int maxIteration;
	public int samplingGap;
	public int testBatch;

	// regularization
	// p(c|z)
	public double topicCoinEntropyMean;
	public double topicCoinEntropyVariance;
	private double[] topicCoinEntropies;
	private double[] topicCoinExpParams;
	private double[][] userTopicCoinEntropies;
	private double[][] userTopicCoinExpParams;
	// p(r|z)
	public double topicRealmEntropyMean;
	public double topicRealmEntropyVariance;
	private double[] topicRealmEntropies;
	private double[] topicRealmExpParams;
	// p(z|r)
	public double realmTopicEntropyMean;
	public double realmTopicEntropyVariance;
	private double[] realmTopicEntropies;
	private double[] realmTopicExpParams;

	// utility variables
	private HashMap<String, Integer> userId2Index;
	private HashMap<Integer, String> userIndex2Id;

	private ArrayTool arrayTool = new ArrayTool();
	
	// utility functions
	private static double getAdjustedIntElementsEntropy(int[] elements,
			double currEntropy, int currSum, int adjustedIndex,
			int adjustedAmount, String topORComm) {
		if (currSum == 0) {
			if (adjustedAmount < 0) {
				System.out.println("0\t" + topORComm);
				System.out.println(topORComm);
				System.out.print(elements[0] + "\t");
				for (int i = 1; i < elements.length; i++)
					System.out.print(elements[i] + "\t");
				System.out.println("");

				System.out.println("currEntropy = " + currEntropy);
				System.out.println("currSum = " + currSum);
				System.out.println("adjustedIndex = " + adjustedIndex);
				System.out.println("adjustedAmount = " + adjustedAmount);
				System.exit(-1);
			} else {
				return 0;
			}
		}
		double newSum = currSum + adjustedAmount;
		if (newSum == 0) {
			return 0;
		}
		double entropy = newSum * Math.log(newSum) - currSum
				* Math.log(currSum);
		entropy = entropy + currSum * currEntropy;
		int currVal = elements[adjustedIndex];
		int newVal = currVal + adjustedAmount;
		if (currVal > 0) {
			entropy = entropy + currVal * Math.log(currVal);
		}
		if (newVal > 0) {
			entropy = entropy - newVal * Math.log(newVal);
		}
		entropy = entropy / newSum;
		if (Double.isNaN(entropy)) {
			System.out.println("1\t" + topORComm);
			System.out.print(elements[0] + "\t");
			for (int i = 1; i < elements.length; i++)
				System.out.print(elements[i] + "\t");
			System.out.println("");

			System.out.println("currEntropy = " + currEntropy);
			System.out.println("currSum = " + currSum);
			System.out.println("adjustedIndex = " + adjustedIndex);
			System.out.println("adjustedAmount = " + adjustedAmount);
			System.out.println("entropy = " + entropy);
			System.exit(-1);
		}
		if (entropy - Math.log(elements.length) > 0.01) {
			System.out.println("2\t" + topORComm);
			System.out.print(elements[0] + "\t");
			for (int i = 1; i < elements.length; i++)
				System.out.print(elements[i] + "\t");
			System.out.println("");

			System.out.println("currEntropy = " + currEntropy);
			System.out.println("currSum = " + currSum);
			System.out.println("adjustedIndex = " + adjustedIndex);
			System.out.println("adjustedAmount = " + adjustedAmount);
			System.out.println("entropy = " + entropy);
			System.exit(-1);
		}
		return entropy;
	}

	// prior settings
	private void setPriors() {
		// user topic prior
		alpha = 50.0 / nTopics;
		sum_alpha = 50;

		// realm prior
		tau = 1.0 / nRealms;
		sum_tau = 1;

		// realm conformity prior
		rho = 2;
		sum_rho = 4;

		// realm topic prior
		eta = 50.0 / nTopics;
		sum_eta = 50;

		// topic tweet word prior
		beta = 0.01;
		sum_beta = 0.01 * tweetVocabulary.length;

		// topic behavior word prior
		gamma = 0.01;
		sum_gamma = new double[nBehaviorTypes];
		for (int l = 0; l < nBehaviorTypes; l++)
			sum_gamma[l] = 0.01 * behaviorVocabularies[l].length * 0.01;
	}

	// data
	public void readData() {
		BufferedReader br = null;
		String line = null;
		// read tweet
		try {
			String folderName = dataPath + SystemTool.pathSeparator + "tweet"
					+ SystemTool.pathSeparator + "users";
			File tweetFolder = new File(folderName);
			// read number of users
			int nUser = tweetFolder.listFiles().length;
			users = new User[nUser];

			userId2Index = new HashMap<String, Integer>(nUser);
			userIndex2Id = new HashMap<Integer, String>(nUser);
			int u = -1;
			for (File tweetFile : tweetFolder.listFiles()) {
				u++;
				users[u] = new User();
				// index of the user
				String userId = FilenameUtils.removeExtension(tweetFile
						.getName());
				userId2Index.put(userId, u);
				userIndex2Id.put(u, userId);
				users[u].userID = userId;
				// read the tweet
				int nTweet = 0;
				br = new BufferedReader(new FileReader(
						tweetFile.getAbsolutePath()));
				while (br.readLine() != null) {
					nTweet++;
				}
				br.close();

				users[u].tweets = new Tweet[nTweet];

				br = new BufferedReader(new FileReader(
						tweetFile.getAbsolutePath()));
				int t = -1;
				while ((line = br.readLine()) != null) {
					t++;
					users[u].tweets[t] = new Tweet();
					String[] tokens = line.split(" ");
					users[u].tweets[t].tweetID = tokens[0];
					users[u].tweets[t].batch = Integer.parseInt(tokens[1]);
					users[u].tweets[t].words = new int[tokens.length - 2];
					for (int i = 0; i < tokens.length - 2; i++)
						users[u].tweets[t].words[i] = Integer
								.parseInt(tokens[i + 2]);
				}
				br.close();
			}

			// read tweet vocabulary
			String tweetVocabularyFileName = dataPath
					+ SystemTool.pathSeparator + "tweet"
					+ SystemTool.pathSeparator + "vocabulary.txt";

			br = new BufferedReader(new FileReader(tweetVocabularyFileName));
			int nTweetWord = 0;
			while (br.readLine() != null) {
				nTweetWord++;
			}
			br.close();
			tweetVocabulary = new String[nTweetWord];
			br = new BufferedReader(new FileReader(tweetVocabularyFileName));
			while ((line = br.readLine()) != null) {
				String[] tokens = line.split(",");
				int index = Integer.parseInt(tokens[0]);
				tweetVocabulary[index] = tokens[1];
			}
			br.close();

		} catch (Exception e) {
			System.out.println("Error in reading tweet from file!");
			e.printStackTrace();
			System.exit(0);
		}
		// read behaviors
		try {
			String behaviorMetaFileName = dataPath + SystemTool.pathSeparator
					+ "behaviors.txt";
			br = new BufferedReader(new FileReader(behaviorMetaFileName));
			// get behavior metadata
			line = br.readLine();
			nBehaviorTypes = Integer.parseInt(line);
			behaviorVocabularies = new String[nBehaviorTypes][];
			behaviorNames = new String[nBehaviorTypes];
			behaviorFolders = new String[nBehaviorTypes];
			for (int l = 0; l < nBehaviorTypes; l++) {
				line = br.readLine();
				String[] tokens = line.split(",");
				int bIndex = Integer.parseInt(tokens[0]);
				behaviorNames[bIndex] = tokens[1];
				behaviorFolders[bIndex] = tokens[2];
			}
			br.close();
			for (int l = 0; l < nBehaviorTypes; l++) {
				String behaviorFolder = dataPath + SystemTool.pathSeparator
						+ behaviorFolders[l];
				// training data
				br = new BufferedReader(new FileReader(behaviorFolder
						+ SystemTool.pathSeparator + "trainset.txt"));
				while ((line = br.readLine()) != null) {
					String[] tokens = line.split(" ");
					String userId = tokens[0];
					// System.out.println(behaviorFolders[i] + " " + userId);
					int userIndex = userId2Index.get(userId);
					if (users[userIndex].trainBehaviors == null)
						users[userIndex].trainBehaviors = new Behavior[nBehaviorTypes][];
					users[userIndex].trainBehaviors[l] = new Behavior[tokens.length - 1];
					for (int b = 0; b < tokens.length - 1; b++) {
						users[userIndex].trainBehaviors[l][b] = new Behavior();
						String[] subTokens = tokens[b + 1].split(":");
						users[userIndex].trainBehaviors[l][b].index = Integer
								.parseInt(subTokens[0]);
						users[userIndex].trainBehaviors[l][b].count = Integer
								.parseInt(subTokens[1]);
					}

				}
				br.close();
				// test data
				br = new BufferedReader(new FileReader(behaviorFolder
						+ SystemTool.pathSeparator + "testset.txt"));
				while ((line = br.readLine()) != null) {
					String[] tokens = line.split(" ");
					String userId = tokens[0];
					int userIndex = userId2Index.get(userId);
					if (users[userIndex].testBehaviors == null)
						users[userIndex].testBehaviors = new Behavior[nBehaviorTypes][];
					users[userIndex].testBehaviors[l] = new Behavior[tokens.length - 1];
					for (int b = 0; b < tokens.length - 1; b++) {
						users[userIndex].testBehaviors[l][b] = new Behavior();
						String[] subTokens = tokens[b + 1].split(":");
						users[userIndex].testBehaviors[l][b].index = Integer
								.parseInt(subTokens[0]);
						users[userIndex].testBehaviors[l][b].count = Integer
								.parseInt(subTokens[1]);
					}
				}
				br.close();
				// vocabulary
				br = new BufferedReader(new FileReader(behaviorFolder
						+ SystemTool.pathSeparator + "vocabulary.txt"));
				int nBehaviorWord = 0;
				while (br.readLine() != null) {
					nBehaviorWord++;
				}
				br.close();
				behaviorVocabularies[l] = new String[nBehaviorWord];

				br = new BufferedReader(new FileReader(behaviorFolder
						+ SystemTool.pathSeparator + "vocabulary.txt"));
				while ((line = br.readLine()) != null) {
					String[] tokens = line.split(",");
					int index = Integer.parseInt(tokens[0]);
					behaviorVocabularies[l][index] = tokens[1];
				}
				br.close();
			}

		} catch (Exception e) {
			System.out.println("Error in reading behaviors from file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	// sampling
	private void declareFinalCounts() {
		final_n_cu = new int[2][users.length];
		final_sum_ncu = new int[users.length];
		for (int u = 0; u < users.length; u++) {
			final_n_cu[0][u] = 0;
			final_n_cu[1][u] = 0;
			final_sum_ncu[u] = 0;
		}
		final_n_ru = new int[nRealms][users.length];
		final_sum_nru = new int[users.length];
		for (int u = 0; u < users.length; u++) {
			for (int r = 0; r < nRealms; r++)
				final_n_ru[r][u] = 0;
			final_sum_nru[u] = 0;
		}
		final_n_zu = new int[nTopics][users.length];
		final_sum_nzu = new int[users.length];
		for (int u = 0; u < users.length; u++) {
			for (int z = 0; z < nTopics; z++)
				final_n_zu[z][u] = 0;
			final_sum_nzu[u] = 0;
		}
		final_n_zr = new int[nTopics][nRealms];
		final_sum_nzr = new int[nRealms];
		for (int r = 0; r < nRealms; r++) {
			for (int z = 0; z < nTopics; z++)
				final_n_zr[z][r] = 0;
			final_sum_nzr[r] = 0;
		}
		final_n_wz = new int[tweetVocabulary.length][nTopics];
		final_sum_nwz = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				final_n_wz[w][z] = 0;
			final_sum_nwz[z] = 0;
		}
		final_n_lbz = new int[nBehaviorTypes][][];
		final_sum_nlbz = new int[nBehaviorTypes][];
		for (int i = 0; i < nBehaviorTypes; i++) {
			final_n_lbz[i] = new int[behaviorVocabularies[i].length][nTopics];
			final_sum_nlbz[i] = new int[nTopics];
			for (int z = 0; z < nTopics; z++) {
				for (int b = 0; b < behaviorVocabularies[i].length; b++)
					final_n_lbz[i][b][z] = 0;
				final_sum_nlbz[i][z] = 0;
			}
		}
		final_n_zc = new int[nTopics][2];
		for (int z = 0; z < nTopics; z++) {
			final_n_zc[z][0] = 0;
			final_n_zc[z][1] = 0;
		}
	}

	private void initilize() {
		// init coin and topic for each tweet and each behavior
		for (int u = 0; u < users.length; u++) {
			// tweet
			for (int t = 0; t < users[u].tweets.length; t++) {
				if (users[u].tweets[t].batch == testBatch)
					continue;
				users[u].tweets[t].coin = rand.nextInt(2);
				users[u].tweets[t].realm = rand.nextInt(nRealms);
				users[u].tweets[t].topic = rand.nextInt(nTopics);
			}
			// behaviors
			if (users[u].trainBehaviors == null)
				continue;
			for (int l = 0; l < nBehaviorTypes; l++) {
				if (users[u].trainBehaviors[l] == null)
					continue;
				for (int b = 0; b < users[u].trainBehaviors[l].length; b++) {
					users[u].trainBehaviors[l][b].coins = new int[users[u].trainBehaviors[l][b].count];
					users[u].trainBehaviors[l][b].realms = new int[users[u].trainBehaviors[l][b].count];
					users[u].trainBehaviors[l][b].topics = new int[users[u].trainBehaviors[l][b].count];
					for (int j = 0; j < users[u].trainBehaviors[l][b].count; j++) {
						users[u].trainBehaviors[l][b].coins[j] = rand
								.nextInt(2);
						users[u].trainBehaviors[l][b].realms[j] = rand
								.nextInt(nRealms);
						users[u].trainBehaviors[l][b].topics[j] = rand
								.nextInt(nTopics);
					}
				}
			}
		}
		// declare and initiate counting tables
		// user-coin
		n_cu = new int[2][users.length];
		sum_ncu = new int[users.length];
		for (int u = 0; u < users.length; u++) {
			n_cu[0][u] = 0;
			n_cu[1][u] = 0;
			sum_ncu[u] = 0;
		}
		// user-realm
		n_ru = new int[nRealms][users.length];
		sum_nru = new int[users.length];
		for (int u = 0; u < users.length; u++) {
			for (int r = 0; r < nRealms; r++)
				n_ru[r][u] = 0;
			sum_nru[u] = 0;
		}
		// user-topic
		n_zu = new int[nTopics][users.length];
		sum_nzu = new int[users.length];
		for (int u = 0; u < users.length; u++) {
			for (int z = 0; z < nTopics; z++)
				n_zu[z][u] = 0;
			sum_nzu[u] = 0;
		}
		// realm-topic
		n_zr = new int[nTopics][nRealms];
		sum_nzr = new int[nRealms];
		for (int r = 0; r < nRealms; r++) {
			for (int z = 0; z < nTopics; z++)
				n_zr[z][r] = 0;
			sum_nzr[r] = 0;
		}
		// topic-word
		n_wz = new int[tweetVocabulary.length][nTopics];
		sum_nwz = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				n_wz[w][z] = 0;
			sum_nwz[z] = 0;
		}
		// topic-behavior
		n_lbz = new int[nBehaviorTypes][][];
		sum_nlbz = new int[nBehaviorTypes][];
		for (int l = 0; l < nBehaviorTypes; l++) {
			n_lbz[l] = new int[behaviorVocabularies[l].length][nTopics];
			sum_nlbz[l] = new int[nTopics];
			for (int z = 0; z < nTopics; z++) {
				for (int b = 0; b < behaviorVocabularies[l].length; b++)
					n_lbz[l][b][z] = 0;
				sum_nlbz[l][z] = 0;
			}
		}
		// for sparsity regularization
		if (topicCoinSparityFlag) {// p(c|z)
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				n_zc = new int[nTopics][2];
				for (int z = 0; z < nTopics; z++) {
					for (int c = 0; c < 2; c++) {
						n_zc[z][c] = 0;
					}
				}
			} else {// personal
				n_uzc = new int[users.length][nTopics][2];
				for (int u = 0; u < users.length; u++) {
					for (int z = 0; z < nTopics; z++) {
						for (int c = 0; c < 2; c++) {
							n_uzc[u][z][c] = 0;
						}
					}
				}
			}
		}
		if (topicRealmSparityFlag) {// p(r|z)
			sum_nrz = new int[nTopics];
			for (int z = 0; z < nTopics; z++)
				sum_nrz[z] = 0;
		}
		if (realmTopicSparityFlag) {// p(z|r)
			n_rz = new int[nRealms][nTopics];
			for (int r = 0; r < nRealms; r++) {
				for (int z = 0; z < nTopics; z++) {
					n_rz[r][z] = 0;
				}
			}
		}
		// update counting tables
		for (int u = 0; u < users.length; u++) {
			// tweet
			for (int t = 0; t < users[u].tweets.length; t++) {
				if (users[u].tweets[t].batch == testBatch)
					continue;
				int c = users[u].tweets[t].coin;
				int r = users[u].tweets[t].realm;
				int z = users[u].tweets[t].topic;
				if (topicCoinSparityFlag) {// p(c|z)
					if (globalOrPersonalTopicCoinSparityFlag)// global
						n_zc[z][c]++;
					else
						// personal
						n_uzc[u][z][c]++;
				}
				// user-coin
				n_cu[c][u]++;
				sum_ncu[u]++;
				// user-topic and realm-topic
				if (c == 0) {// based on user interest
					// user-topic
					n_zu[z][u]++;
					sum_nzu[u]++;
				} else {// based on realm interest
					// user-realm
					n_ru[r][u]++;
					sum_nru[u]++;
					// realm-topic
					n_zr[z][r]++;
					sum_nzr[r]++;
					if (topicRealmSparityFlag)// p(r|z)
						sum_nrz[z]++;
					if (realmTopicSparityFlag)// p(z|r)
						n_rz[r][z]++;

				}
				// topic-word
				for (int i = 0; i < users[u].tweets[t].words.length; i++) {
					int w = users[u].tweets[t].words[i];
					n_wz[w][z]++;
					sum_nwz[z]++;
				}
			}
			// behaviors
			if (users[u].trainBehaviors == null)
				continue;
			for (int l = 0; l < nBehaviorTypes; l++) {
				if (users[u].trainBehaviors[l] == null)
					continue;
				for (int b = 0; b < users[u].trainBehaviors[l].length; b++) {
					int bw = users[u].trainBehaviors[l][b].index;
					for (int j = 0; j < users[u].trainBehaviors[l][b].count; j++) {
						int c = users[u].trainBehaviors[l][b].coins[j];
						int r = users[u].trainBehaviors[l][b].realms[j];
						int z = users[u].trainBehaviors[l][b].topics[j];
						if (topicCoinSparityFlag) {// p(c|z)
							if (globalOrPersonalTopicCoinSparityFlag)// global
								n_zc[z][c]++;
							else
								// personal
								n_uzc[u][z][c]++;
						}
						// user-coin
						n_cu[c][u]++;
						sum_ncu[u]++;
						// user-topic and realm-topic
						if (c == 0) {// based on user interest
							// user-topic
							n_zu[z][u]++;
							sum_nzu[u]++;
						} else {// based on realm
							// user-realm
							n_ru[r][u]++;
							sum_nru[u]++;
							// realm-topic
							n_zr[z][r]++;
							sum_nzr[r]++;
							if (topicRealmSparityFlag)// p(r|z)
								sum_nrz[z]++;
							if (realmTopicSparityFlag)// p(z|r)
								n_rz[r][z]++;
						}
						// behavior word - topic
						n_lbz[l][bw][z]++;
						sum_nlbz[l][z]++;
					}
				}
			}
		}
		// sparsity condition
		if (topicCoinSparityFlag) {// p(c|z)
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				topicCoinEntropies = new double[nTopics];
				topicCoinExpParams = new double[nTopics];
				for (int z = 0; z < nTopics; z++) {
					topicCoinEntropies[z] = arrayTool
							.getIntElementsEntropy(n_zc[z]);
					topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
							- topicCoinEntropyMean, 2)
							/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				}
			} else {// personal
				userTopicCoinEntropies = new double[users.length][nTopics];
				userTopicCoinExpParams = new double[users.length][nTopics];
				for (int u = 0; u < users.length; u++) {
					for (int z = 0; z < nTopics; z++) {
						userTopicCoinEntropies[u][z] = arrayTool
								.getIntElementsEntropy(n_uzc[u][z]);
						userTopicCoinExpParams[u][z] = -Math.pow(
								userTopicCoinEntropies[u][z]
										- topicRealmEntropyMean, 2)
								/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
					}
				}
			}
		}
		if (topicRealmSparityFlag) {// p(r|z)
			topicRealmEntropies = new double[nTopics];
			topicRealmExpParams = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				topicRealmEntropies[z] = arrayTool
						.getIntElementsEntropy(n_zr[z]);
				topicRealmExpParams[z] = -Math.pow(topicRealmEntropies[z]
						- topicRealmEntropyMean, 2)
						/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
			}
		}
		if (realmTopicSparityFlag) { // p(z|r)
			realmTopicEntropies = new double[nRealms];
			realmTopicExpParams = new double[nRealms];
			for (int r = 0; r < nRealms; r++) {
				realmTopicEntropies[r] = arrayTool
						.getIntElementsEntropy(n_rz[r]);
				realmTopicExpParams[r] = -Math.pow(realmTopicEntropies[r]
						- realmTopicEntropyMean, 2)
						/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
			}
		}
	}

	private void sampleTweetCoinRealm(int u, int t) {
		// jointly sample coin and realm for tweet number t of user number u

		// get current coin
		int currc = users[u].tweets[t].coin;
		// get current realm
		int currr = -1;
		if (currc == 1) {
			currr = users[u].tweets[t].realm;
		}
		// get current topic
		int z = users[u].tweets[t].topic;
		// user-topic-coin & topic-coin
		if (topicCoinSparityFlag) {// p(c|z)
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				topicCoinEntropies[z] = getAdjustedIntElementsEntropy(n_zc[z],
						topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1], currc,
						-1, "current-tweet-CoinRealm:zc");
				topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
						- topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_zc[z][currc]--;
			} else {// personal
				userTopicCoinEntropies[u][z] = getAdjustedIntElementsEntropy(
						n_uzc[u][z], userTopicCoinEntropies[u][z],
						n_uzc[u][z][0] + n_uzc[u][z][1], currc, -1,
						"current-tweet-CoinRealm:uzc");
				userTopicCoinExpParams[u][z] = -Math.pow(
						userTopicCoinEntropies[u][z] - topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_uzc[u][z][currc]--;
			}
		}
		// user-coin
		n_cu[currc][u]--;
		sum_ncu[u]--;
		if (currc == 0) {
			// user-topic
			n_zu[z][u]--;
			sum_nzu[u]--;
		} else {
			// user-realm
			n_ru[currr][u]--;
			sum_nru[u]--;
			// realm-topic
			if (topicRealmSparityFlag) {// p(r|z)
				topicRealmEntropies[z] = getAdjustedIntElementsEntropy(n_zr[z],
						topicRealmEntropies[z], sum_nrz[z], currr, -1,
						"current-tweet-CoinRealm:zr");
				topicRealmExpParams[z] = -Math.pow(topicRealmEntropies[z]
						- topicRealmEntropyMean, 2)
						/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
				sum_nrz[z]--;
			}
			if (realmTopicSparityFlag) {// p(z|r)
				realmTopicEntropies[currr] = getAdjustedIntElementsEntropy(
						n_rz[currr], realmTopicEntropies[currr],
						sum_nzr[currr], z, -1, "current-tweet-CoinRealm:rz");
				realmTopicExpParams[currr] = -Math.pow(
						realmTopicEntropies[currr] - realmTopicEntropyMean, 2)
						/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
				n_rz[currr][z]--;
			}
			n_zr[z][currr]--;
			sum_nzr[currr]--;
		}
		// *********************************c=0*********************************
		// probability that c = 0 given recent counts
		double p_0 = (n_cu[0][u] + rho) / (sum_ncu[u] + sum_rho);
		// probability of z given c = 0
		p_0 = p_0 * (n_zu[z][u] + alpha) / (sum_nzu[u] + sum_alpha);
		// sparsity regularization
		if (topicCoinSparityFlag) {// p(c|z)
			double newZCEntropy = 0;
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				newZCEntropy = getAdjustedIntElementsEntropy(n_zc[z],
						topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1], 0, 1,
						"tweet-CoinRealm:zc=0,global");
			} else {// personal
				newZCEntropy = getAdjustedIntElementsEntropy(n_uzc[u][z],
						userTopicCoinEntropies[u][z], n_uzc[u][z][0]
								+ n_uzc[u][z][1], 0, 1,
						"tweet-CoinRealm:zc=0,personal");
			}
			double zcExpParam = -Math.pow(newZCEntropy - topicCoinEntropyMean,
					2)
					/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
			p_0 = p_0 * Math.exp(zcExpParam);
		}
		if (topicRealmSparityFlag) {// p(r|z) sparsity condition
			p_0 = p_0 * Math.exp(topicRealmExpParams[z]);
		}
		if (realmTopicSparityFlag) {// p(z|r) sparsity condition
			// p_0 = p_0 * Math.exp(sumRealmTopicExpParams);
			// the sum is canceled
		}
		// *********************************c=0*********************************
		// probability that c = 1 given recent counts
		double p_1 = (n_cu[1][u] + rho) / (sum_ncu[u] + sum_rho);
		// sparsity condition
		if (topicCoinSparityFlag) { // p(c|z)
			double newZCEntropy = 0;
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				newZCEntropy = getAdjustedIntElementsEntropy(n_zc[z],
						topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1], 1, 1,
						"tweet-CoinRealm:zc=1,global");
			} else {// personal
				newZCEntropy = getAdjustedIntElementsEntropy(n_uzc[u][z],
						userTopicCoinEntropies[u][z], n_uzc[u][z][0]
								+ n_uzc[u][z][1], 1, 1,
						"tweet-CoinRealm:zc=1,personal");
			}
			double zcExpParam = -Math.pow(newZCEntropy - topicCoinEntropyMean,
					2)
					/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
			p_1 = p_1 * Math.exp(zcExpParam);
		}
		// probability of (z,r) given c = 1
		double[] pRealms = new double[nRealms];
		for (int r = 0; r < nRealms; r++) {
			// probability of r given recent counts
			pRealms[r] = (n_ru[r][u] + tau) / (sum_nru[u] + sum_tau);
			// probability of z given r
			pRealms[r] = pRealms[r] * (n_zr[z][r] + eta)
					/ (sum_nzr[r] + sum_eta);
			// sparsity condition
			if (topicRealmSparityFlag) {// p(r|z)
				double newZREntropy = getAdjustedIntElementsEntropy(n_zr[z],
						topicRealmEntropies[z], sum_nrz[z], r, 1,
						"tweet-CoinRealm:zr");
				double zrExpParam = -Math.pow(newZREntropy
						- topicRealmEntropyMean, 2)
						/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
				pRealms[r] = pRealms[r] * Math.exp(zrExpParam);
			}
			if (realmTopicSparityFlag) {// p(z|r)
				double newRZEntropy = getAdjustedIntElementsEntropy(n_rz[r],
						realmTopicEntropies[r], sum_nzr[r], z, 1,
						"tweet-CoinRealm:rz");
				double rzExpParam = -Math.pow(newRZEntropy
						- realmTopicEntropyMean, 2)
						/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
				pRealms[r] = pRealms[r]
						* Math.exp(rzExpParam - realmTopicExpParams[r]);
			}
		}

		double sump = p_0;
		for (int r = 0; r < nRealms; r++) {
			pRealms[r] = pRealms[r] * p_1;
			// cumulative distribution
			pRealms[r] += sump;
			sump = pRealms[r];
		}

		sump = rand.nextDouble() * sump;
		int c = 0;
		if (sump > p_0)
			c = 1;
		// the coin
		users[u].tweets[t].coin = c;
		// user-coin
		n_cu[c][u]++;
		sum_ncu[u]++;
		// user-topic-coin & topic-coin
		if (topicCoinSparityFlag) {// p(c|z)
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				topicCoinEntropies[z] = getAdjustedIntElementsEntropy(n_zc[z],
						topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1], c, 1,
						"new-tweet-CoinRealm:zv");
				topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
						- topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_zc[z][c]++;
			} else {// personal
				userTopicCoinEntropies[u][z] = getAdjustedIntElementsEntropy(
						n_uzc[u][z], userTopicCoinEntropies[u][z],
						n_uzc[u][z][0] + n_uzc[u][z][1], c, 1,
						"new-tweet-CoinRealm:uzv");
				userTopicCoinExpParams[u][z] = -Math.pow(
						userTopicCoinEntropies[u][z] - topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_uzc[u][z][c]++;
			}
		}
		if (c == 0) {
			// user-topic
			n_zu[z][u]++;
			sum_nzu[u]++;
		} else {
			for (int r = 0; r < nRealms; r++) {
				if (sump > pRealms[r])
					continue;
				// the realm
				users[u].tweets[t].realm = r;
				// user-realm
				n_ru[r][u]++;
				sum_nru[u]++;
				// realm-topic
				if (topicRealmSparityFlag) {
					topicRealmEntropies[z] = getAdjustedIntElementsEntropy(
							n_zr[z], topicRealmEntropies[z], sum_nrz[z], r, 1,
							"new-tweet-CoinRealm:zr");
					topicRealmExpParams[z] = -Math.pow(topicRealmEntropies[z]
							- topicRealmEntropyMean, 2)
							/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
					sum_nrz[z]++;
				}
				if (realmTopicSparityFlag) {
					realmTopicEntropies[r] = getAdjustedIntElementsEntropy(
							n_rz[r], realmTopicEntropies[r], sum_nzr[r], z, 1,
							"new-tweet-CoinRealm:rz");
					realmTopicExpParams[r] = -Math.pow(realmTopicEntropies[r]
							- realmTopicEntropyMean, 2)
							/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
					n_rz[r][z]++;
				}
				n_zr[z][r]++;
				sum_nzr[r]++;
				return;
			}
			System.out.println("bug in sampleTweetCoin");
			System.exit(-1);
		}
	}

	private void sampleTweetTopic(int u, int t) {
		// sample the topic for tweet number t of user number u
		// get current topic
		int currz = users[u].tweets[t].topic;
		// get current coin
		int c = users[u].tweets[t].coin;
		// user-topic-coin & topic-coin
		if (topicCoinSparityFlag) {// p(c|z)
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				topicCoinEntropies[currz] = getAdjustedIntElementsEntropy(
						n_zc[currz], topicCoinEntropies[currz], n_zc[currz][0]
								+ n_zc[currz][1], c, -1,
						"current-tweet-Topic:zc");
				topicCoinExpParams[currz] = -Math.pow(topicCoinEntropies[currz]
						- topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_zc[currz][c]--;
			} else {// personal
				userTopicCoinEntropies[u][currz] = getAdjustedIntElementsEntropy(
						n_uzc[u][currz], userTopicCoinEntropies[u][currz],
						n_uzc[u][currz][0] + n_uzc[u][currz][1], c, -1,
						"current-tweet-Topic:uzc");
				userTopicCoinExpParams[u][currz] = -Math
						.pow(userTopicCoinEntropies[u][currz]
								- topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_uzc[u][currz][c]--;
			}
		}

		if (c == 0) {
			// sampling based on user interest

			// user-topic
			n_zu[currz][u]--;
			sum_nzu[u]--;
			// topic-word
			for (int i = 0; i < users[u].tweets[t].words.length; i++) {
				int w = users[u].tweets[t].words[i];
				n_wz[w][currz]--;
				sum_nwz[currz]--;
			}
			// ***********************************************************
			double sump = 0;
			double[] p = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				// probability of z given recent counts
				p[z] = (n_zu[z][u] + alpha) / (sum_nzu[u] + sum_alpha);
				// probability of tweet content given z
				for (int i = 0; i < users[u].tweets[t].words.length; i++) {
					int w = users[u].tweets[t].words[i];
					p[z] = p[z] * (n_wz[w][z] + beta) / (sum_nwz[z] + sum_beta);
				}
				// sparsity condition
				if (topicCoinSparityFlag) {// p(c|z)
					double newZCEntropy = 0;
					if (globalOrPersonalTopicCoinSparityFlag) {// global
						newZCEntropy = getAdjustedIntElementsEntropy(n_zc[z],
								topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1],
								0, 1, "tweet-Topic:zc=0,global");
					} else {// personal
						newZCEntropy = getAdjustedIntElementsEntropy(
								n_uzc[u][z], userTopicCoinEntropies[u][z],
								n_uzc[u][z][0] + n_uzc[u][z][1], 0, 1,
								"tweet-Topic:zc=0,user-specific");
					}
					double zcExpParam = -Math.pow(newZCEntropy
							- topicCoinEntropyMean, 2)
							/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
					if (globalOrPersonalTopicCoinSparityFlag)
						p[z] = p[z]
								* Math.exp(zcExpParam - topicCoinExpParams[z]);
					else
						p[z] = p[z]
								* Math.exp(zcExpParam
										- userTopicCoinExpParams[u][z]);
				}
				// cumulative
				p[z] = sump + p[z];
				sump = p[z];
			}
			sump = rand.nextDouble() * sump;
			for (int z = 0; z < nTopics; z++) {
				if (sump > p[z])
					continue;
				// the topic
				users[u].tweets[t].topic = z;
				// user-topic
				n_zu[z][u]++;
				sum_nzu[u]++;
				// user-topic-coin & topic-coin
				if (topicCoinSparityFlag) {// p(c|z)
					if (globalOrPersonalTopicCoinSparityFlag) {// global
						topicCoinEntropies[z] = getAdjustedIntElementsEntropy(
								n_zc[z], topicCoinEntropies[z], n_zc[z][0]
										+ n_zc[z][1], 0, 1,
								"new-tweet-Topic:zc");
						topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
								- topicCoinEntropyMean, 2)
								/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
						n_zc[z][0]++;
					} else {// personal
						userTopicCoinEntropies[u][z] = getAdjustedIntElementsEntropy(
								n_uzc[u][z], userTopicCoinEntropies[u][z],
								n_uzc[u][z][0] + n_uzc[u][z][1], 0, 1,
								"new-tweet-Topic:uzc");
						userTopicCoinExpParams[u][z] = -Math.pow(
								userTopicCoinEntropies[u][z]
										- topicCoinEntropyMean, 2)
								/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
						n_uzc[u][z][0]++;
					}
				}
				// topic-word
				for (int i = 0; i < users[u].tweets[t].words.length; i++) {
					int w = users[u].tweets[t].words[i];
					n_wz[w][z]++;
					sum_nwz[z]++;
				}
				return;
			}
			System.out.println("bug in sampleTweetTopic");
			System.exit(-1);
		} else {
			// sampling based on realm interest
			// get current community
			int r = users[u].tweets[t].realm;
			// realm-topic
			if (topicRealmSparityFlag) {// p(r|z)
				topicRealmEntropies[currz] = getAdjustedIntElementsEntropy(
						n_zr[currz], topicRealmEntropies[currz],
						sum_nrz[currz], r, -1, "current-tweet-Topic:c=1,zr");
				topicRealmExpParams[currz] = -Math.pow(
						topicRealmEntropies[currz] - topicRealmEntropyMean, 2)
						/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
				sum_nrz[currz]--;
			}
			if (realmTopicSparityFlag) {// p(z|r)
				realmTopicEntropies[r] = getAdjustedIntElementsEntropy(n_rz[r],
						realmTopicEntropies[r], sum_nzr[r], currz, -1,
						"current-tweet-Topic:c=1,rz");
				realmTopicExpParams[r] = -Math.pow(realmTopicEntropies[r]
						- realmTopicEntropyMean, 2)
						/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
				n_rz[r][currz]--;
			}
			n_zr[currz][r]--;
			sum_nzr[r]--;
			// topic-word
			for (int i = 0; i < users[u].tweets[t].words.length; i++) {
				int w = users[u].tweets[t].words[i];
				n_wz[w][currz]--;
				sum_nwz[currz]--;
			}
			// ***********************************************************
			double sump = 0;
			double[] p = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				// probability of z given recent counts
				p[z] = (n_zr[z][r] + eta) / (sum_nzr[r] + sum_eta);
				// probability of tweet content given z
				for (int i = 0; i < users[u].tweets[t].words.length; i++) {
					int w = users[u].tweets[t].words[i];
					p[z] = p[z] * (n_wz[w][z] + beta) / (sum_nwz[z] + sum_beta);
				}
				// sparsity condition
				if (topicCoinSparityFlag) {// p(c|z)
					double newZCEntropy = 0;
					if (globalOrPersonalTopicCoinSparityFlag) {// global
						newZCEntropy = getAdjustedIntElementsEntropy(n_zc[z],
								topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1],
								1, 1, "tweet-Topic:zc=1,global");
					} else {// user specific sparsity condition
						newZCEntropy = getAdjustedIntElementsEntropy(
								n_uzc[u][z], userTopicCoinEntropies[u][z],
								n_uzc[u][z][0] + n_uzc[u][z][1], 1, 1,
								"tweet-Topic:zc=1,user-specific");
					}
					double zcExpParam = -Math.pow(topicCoinEntropyMean
							- newZCEntropy, 2)
							/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
					if (globalOrPersonalTopicCoinSparityFlag)
						p[z] = p[z]
								* Math.exp(zcExpParam - topicCoinExpParams[z]);
					else
						p[z] = p[z]
								* Math.exp(zcExpParam
										- userTopicCoinExpParams[u][z]);
				}
				if (topicRealmSparityFlag) {// p(r|z)
					double newZREntropy = getAdjustedIntElementsEntropy(
							n_zr[z], topicRealmEntropies[z], sum_nrz[z], r, 1,
							"tweet-Topic:c=1,zr");
					double zrExpParam = -Math.pow(newZREntropy
							- topicRealmEntropyMean, 2)
							/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
					p[z] = p[z] * Math.exp(zrExpParam - topicRealmExpParams[z]);
				}
				if (realmTopicSparityFlag) {// p(z|r)
					double newRZEntropy = getAdjustedIntElementsEntropy(
							n_rz[r], realmTopicEntropies[r], sum_nzr[r], z, 1,
							"tweet-Topic:c=1,rz");
					double rzExpParam = -Math.pow(newRZEntropy
							- realmTopicEntropyMean, 2)
							/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
					p[z] = p[z] * Math.exp(rzExpParam);
				}

				// cumulative
				p[z] = p[z] + sump;
				sump = p[z];
			}
			sump = rand.nextDouble() * sump;
			for (int z = 0; z < nTopics; z++) {
				if (sump > p[z])
					continue;
				// the-topic
				users[u].tweets[t].topic = z;
				// user-topic-coin & topic-coin
				if (topicCoinSparityFlag) {// p(c|z)
					if (globalOrPersonalTopicCoinSparityFlag) {// global
						topicCoinEntropies[z] = getAdjustedIntElementsEntropy(
								n_zc[z], topicCoinEntropies[z], n_zc[z][0]
										+ n_zc[z][1], 1, 1,
								"new-tweet-Topic:c=1,zc");
						topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
								- topicCoinEntropyMean, 2)
								/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
						n_zc[z][1]++;
					} else {// personal
						userTopicCoinEntropies[u][z] = getAdjustedIntElementsEntropy(
								n_uzc[u][z], userTopicCoinEntropies[u][z],
								n_uzc[u][z][0] + n_uzc[u][z][1], 1, 1,
								"new-tweet-Topic:c=1,uzc");
						userTopicCoinExpParams[u][z] = -Math.pow(
								userTopicCoinEntropies[u][z]
										- topicCoinEntropyMean, 2)
								/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
						n_uzc[u][z][1]++;
					}
				}

				// realm-topic
				if (topicRealmSparityFlag) {// p(r|z)
					topicRealmEntropies[z] = getAdjustedIntElementsEntropy(
							n_zr[z], topicRealmEntropies[z], sum_nrz[z], r, 1,
							"new-tweet-Topic:c=1,zr");
					topicRealmExpParams[z] = -Math.pow(topicRealmEntropies[z]
							- topicRealmEntropyMean, 2)
							/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
					sum_nrz[z]++;
				}
				if (realmTopicSparityFlag) {// p(z|r)
					realmTopicEntropies[r] = getAdjustedIntElementsEntropy(
							n_rz[r], realmTopicEntropies[r], sum_nzr[r], z, 1,
							"new-tweet-Topic:c=1,rz");
					realmTopicExpParams[r] = -Math.pow(realmTopicEntropies[r]
							- realmTopicEntropyMean, 2)
							/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
					n_rz[r][z]++;
				}
				n_zr[z][r]++;
				sum_nzr[r]++;
				// topic-word
				for (int i = 0; i < users[u].tweets[t].words.length; i++) {
					int w = users[u].tweets[t].words[i];
					n_wz[w][z]++;
					sum_nwz[z]++;
				}
				return;
			}
			System.out.println("bug in sampleTweetTopic v = 1");
			System.exit(-1);
		}
	}

	private void sampleBehaviorCoinRealm(int u, int l, int b, int j) {
		// jointly sample coin and realm for the type-l behavior number j/count
		// at index b of user number u

		// get current coin
		int currc = users[u].trainBehaviors[l][b].coins[j];
		// get current realm
		int currr = -1;
		if (currc == 1) {
			currr = users[u].trainBehaviors[l][b].realms[j];
		}
		// get current topic
		int z = users[u].trainBehaviors[l][b].topics[j];
		// user-topic-coin & topic-coin
		if (topicCoinSparityFlag) {// p(c|z)
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				topicCoinEntropies[z] = getAdjustedIntElementsEntropy(n_zc[z],
						topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1], currc,
						-1, "current-behavior-CoinRealm:zc");
				topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
						- topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_zc[z][currc]--;
			} else {// personal
				userTopicCoinEntropies[u][z] = getAdjustedIntElementsEntropy(
						n_uzc[u][z], userTopicCoinEntropies[u][z],
						n_uzc[u][z][0] + n_uzc[u][z][1], currc, -1,
						"current-behavior-CoinRealm:uzc");
				userTopicCoinExpParams[u][z] = -Math.pow(
						userTopicCoinEntropies[u][z] - topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_uzc[u][z][currc]--;
			}
		}

		// user-coin
		n_cu[currc][u]--;
		sum_ncu[u]--;
		if (currc == 0) {
			// user-topic
			n_zu[z][u]--;
			sum_nzu[u]--;
		} else {
			// user-realm
			n_ru[currr][u]--;
			sum_nru[u]--;
			// realm-topic
			if (topicRealmSparityFlag) {// p(r|z)
				topicRealmEntropies[z] = getAdjustedIntElementsEntropy(n_zr[z],
						topicRealmEntropies[z], sum_nrz[z], currr, -1,
						"current-behavior-CoinRealm:zr");
				topicRealmExpParams[z] = -Math.pow(topicRealmEntropies[z]
						- topicRealmEntropyMean, 2)
						/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
				sum_nrz[z]--;
			}
			if (realmTopicSparityFlag) {// p(z|r)
				realmTopicEntropies[currr] = getAdjustedIntElementsEntropy(
						n_rz[currr], realmTopicEntropies[currr],
						sum_nzr[currr], z, -1, "current-behavior-CoinRealm:rz");
				realmTopicExpParams[currr] = -Math.pow(
						realmTopicEntropies[currr] - realmTopicEntropyMean, 2)
						/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
				n_rz[currr][z]--;
			}
			n_zr[z][currr]--;
			sum_nzr[currr]--;
		}
		// **************************************c=0*****************************
		// probability that c = 0 given recent counts
		double p_0 = (n_cu[0][u] + rho) / (sum_ncu[u] + sum_rho);
		// probability of z given c = 0
		p_0 = p_0 * (n_zu[z][u] + alpha) / (sum_nzu[u] + sum_alpha);
		// sparsity regularization
		if (topicCoinSparityFlag) {// p(c|z)
			double newZCEntropy = 0;
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				newZCEntropy = getAdjustedIntElementsEntropy(n_zc[z],
						topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1], 0, 1,
						"behavior-CoinRealm:zv=0,global");
			} else {// personal
				newZCEntropy = getAdjustedIntElementsEntropy(n_uzc[u][z],
						userTopicCoinEntropies[u][z], n_uzc[u][z][0]
								+ n_uzc[u][z][1], 0, 1,
						"behavior-CoinRealm:zc=0,user-specific");
			}
			double zcExpParam = -Math.pow(newZCEntropy - topicCoinEntropyMean,
					2)
					/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
			p_0 = p_0 * Math.exp(zcExpParam);
		}
		if (topicRealmSparityFlag) {// p(r|z)
			p_0 = p_0 * Math.exp(topicRealmExpParams[z]);
		}
		if (realmTopicSparityFlag) {// p(z|r)
			// p_0 = p_0 * Math.exp(sumRealmTopicExpParams);
			// the sum is canceled
		}
		// **************************************c=1*****************************
		// probability that c = 1 given recent counts
		double p_1 = (n_cu[1][u] + rho) / (sum_ncu[u] + sum_rho);
		// sparsity condition
		if (topicCoinSparityFlag) { // p(c|z) sparsity condition
			double newZCEntropy = 0;
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				newZCEntropy = getAdjustedIntElementsEntropy(n_zc[z],
						topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1], 1, 1,
						"behavior-CoinRealm:zc=1,global");
			} else {// personal
				newZCEntropy = getAdjustedIntElementsEntropy(n_uzc[u][z],
						userTopicCoinEntropies[u][z], n_uzc[u][z][0]
								+ n_uzc[u][z][1], 1, 1,
						"behavior-CoinRealm:zv=1,user-specific");
			}
			double zcExpParam = -Math.pow(newZCEntropy - topicCoinEntropyMean,
					2)
					/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
			p_1 = p_1 * Math.exp(zcExpParam);
		}
		// probability of (z,r) given c = 1
		double[] pRealms = new double[nRealms];
		for (int r = 0; r < nRealms; r++) {
			// probability of r given recent counts
			pRealms[r] = (n_ru[r][u] + tau) / (sum_nru[u] + sum_tau);
			// probability of z given r
			pRealms[r] = pRealms[r] * (n_zr[z][r] + eta)
					/ (sum_nzr[r] + sum_eta);
			// sparsity condition
			if (topicRealmSparityFlag) {// p(r|z)
				double newZREntropy = getAdjustedIntElementsEntropy(n_zr[z],
						topicRealmEntropies[z], sum_nrz[z], r, 1,
						"behavior-CoinRealm:zr");
				double zrExpParam = -Math.pow(newZREntropy
						- topicRealmEntropyMean, 2)
						/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
				pRealms[r] = pRealms[r] * Math.exp(zrExpParam);
			}
			if (realmTopicSparityFlag) {// p(z|r)
				double newRZEntropy = getAdjustedIntElementsEntropy(n_rz[r],
						realmTopicEntropies[r], sum_nzr[r], z, 1,
						"behavior-CoinRealm:rz");
				double rzExpParam = -Math.pow(realmTopicEntropyMean
						- newRZEntropy, 2)
						/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
				pRealms[r] = pRealms[r]
						* Math.exp(rzExpParam - realmTopicExpParams[r]);
			}
		}

		double sump = p_0;
		for (int r = 0; r < nRealms; r++) {
			pRealms[r] = pRealms[r] * p_1;
			// cumulative distribution
			pRealms[r] += sump;
			sump = pRealms[r];
		}

		sump = rand.nextDouble() * sump;
		int c = 0;
		if (sump > p_0)
			c = 1;
		// the coin
		users[u].trainBehaviors[l][b].coins[j] = c;
		// user-coin
		n_cu[c][u]++;
		sum_ncu[u]++;
		// user-topic-coin & topic-coin
		if (topicCoinSparityFlag) {
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				topicCoinEntropies[z] = getAdjustedIntElementsEntropy(n_zc[z],
						topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1], c, 1,
						"new-behavior-CoinRealm:zv");
				topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
						- topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_zc[z][c]++;
			} else {// personal
				userTopicCoinEntropies[u][z] = getAdjustedIntElementsEntropy(
						n_uzc[u][z], userTopicCoinEntropies[u][z],
						n_uzc[u][z][0] + n_uzc[u][z][1], c, 1,
						"new-behavior-CoinRealm:uzv");
				userTopicCoinExpParams[u][z] = -Math.pow(
						userTopicCoinEntropies[u][z] - topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_uzc[u][z][c]++;
			}
		}
		if (c == 0) {
			// user-topic
			n_zu[z][u]++;
			sum_nzu[u]++;
		} else {
			for (int r = 0; r < nRealms; r++) {
				if (sump > pRealms[r])
					continue;
				// the realm
				users[u].trainBehaviors[l][b].realms[j] = r;
				// user-realm
				n_ru[r][u]++;
				sum_nru[u]++;
				// realm-topic
				if (topicRealmSparityFlag) {// p(r|z)
					topicRealmEntropies[z] = getAdjustedIntElementsEntropy(
							n_zr[z], topicRealmEntropies[z], sum_nrz[z], r, 1,
							"new-behavior-CoinRealm:zr");
					topicRealmExpParams[z] = -Math.pow(topicRealmEntropies[z]
							- topicRealmEntropyMean, 2)
							/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
					sum_nrz[z]++;
				}
				if (realmTopicSparityFlag) {// p(z|r)
					realmTopicEntropies[r] = getAdjustedIntElementsEntropy(
							n_rz[r], realmTopicEntropies[r], sum_nzr[r], z, 1,
							"new-behavior-CoinRealm:gz");
					realmTopicExpParams[r] = -Math.pow(realmTopicEntropies[r]
							- realmTopicEntropyMean, 2)
							/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
					n_rz[r][z]++;
				}
				n_zr[z][r]++;
				sum_nzr[r]++;
				return;
			}
			System.out.println("bug in sampleBehaviorCoin");
			System.exit(-1);
		}
	}

	private void sampleBehaviorTopic(int u, int l, int b, int j) {
		// sample the topic for the type-l behavior number j/count at index
		// b of user number u
		int currz = users[u].trainBehaviors[l][b].topics[j];
		// get current coin
		int c = users[u].trainBehaviors[l][b].coins[j];
		// user-topic-coin & topic-coin
		if (topicCoinSparityFlag) {// p(c|z)
			if (globalOrPersonalTopicCoinSparityFlag) {// global
				topicCoinEntropies[currz] = getAdjustedIntElementsEntropy(
						n_zc[currz], topicCoinEntropies[currz], n_zc[currz][0]
								+ n_zc[currz][1], c, -1,
						"current-behavior-Topic:zv");
				topicCoinExpParams[currz] = -Math.pow(topicCoinEntropies[currz]
						- topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_zc[currz][c]--;
			} else {// personal
				userTopicCoinEntropies[u][currz] = getAdjustedIntElementsEntropy(
						n_uzc[u][currz], userTopicCoinEntropies[u][currz],
						n_uzc[u][currz][0] + n_uzc[u][currz][1], c, -1,
						"current-behavior-Topic:uzc");
				userTopicCoinExpParams[u][currz] = -Math
						.pow(userTopicCoinEntropies[u][currz]
								- topicCoinEntropyMean, 2)
						/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
				n_uzc[u][currz][c]--;
			}
		}
		if (c == 0) {
			// sampling based on user interest

			// user-topic
			n_zu[currz][u]--;
			sum_nzu[u]--;
			// topic-behavior
			int bw = users[u].trainBehaviors[l][b].index;
			n_lbz[l][bw][currz]--;
			sum_nlbz[l][currz]--;
			//*************************************************************
			double sump = 0;
			double[] p = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				p[z] = (n_zu[z][u] + alpha) / (sum_nzu[u] + sum_alpha);
				p[z] = p[z] * (n_lbz[l][bw][z] + gamma)
						/ (sum_nlbz[l][z] + sum_gamma[l]);
				// sparsity condition
				if (topicCoinSparityFlag) {// p(c|z)
					double newZCEntropy = 0;
					if (globalOrPersonalTopicCoinSparityFlag) {// global
						newZCEntropy = getAdjustedIntElementsEntropy(n_zc[z],
								topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1],
								0, 1, "behavior-Topic:zc=0,global");
					} else {// personal
						newZCEntropy = getAdjustedIntElementsEntropy(
								n_uzc[u][z], userTopicCoinEntropies[u][z],
								n_uzc[u][z][0] + n_uzc[u][z][1], 0, 1,
								"behavior-Topic:zc=0,personal");
					}
					double zcExpParam = -Math.pow(topicCoinEntropyMean
							- newZCEntropy, 2)
							/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
					if (globalOrPersonalTopicCoinSparityFlag)
						p[z] = p[z]
								* Math.exp(zcExpParam - topicCoinExpParams[z]);
					else
						p[z] = p[z]
								* Math.exp(zcExpParam
										- userTopicCoinExpParams[u][z]);
				}
				// cumulative
				p[z] = p[z] + sump;
				sump = p[z];
			}
			sump = rand.nextDouble() * sump;
			for (int z = 0; z < nTopics; z++) {
				if (sump > p[z])
					continue;
				// the topic
				users[u].trainBehaviors[l][b].topics[j] = z;
				// user-topic
				n_zu[z][u]++;
				sum_nzu[u]++;
				// user-topic-coin & topic-coin
				if (topicCoinSparityFlag) {// p(c|z)
					if (globalOrPersonalTopicCoinSparityFlag) {// global
						topicCoinEntropies[z] = getAdjustedIntElementsEntropy(
								n_zc[z], topicCoinEntropies[z], n_zc[z][0]
										+ n_zc[z][1], 0, 1,
								"new-behavior-Topic:zc");
						topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
								- topicCoinEntropyMean, 2)
								/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
						n_zc[z][0]++;
					} else {// personal
						userTopicCoinEntropies[u][z] = getAdjustedIntElementsEntropy(
								n_uzc[u][z], userTopicCoinEntropies[u][z],
								n_uzc[u][z][0] + n_uzc[u][z][1], 0, 1,
								"new-behavior-Topic:uzc");
						userTopicCoinExpParams[u][z] = -Math.pow(
								userTopicCoinEntropies[u][z]
										- topicCoinEntropyMean, 2)
								/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
						n_uzc[u][z][0]++;
					}
				}
				// topic-behavior
				n_lbz[l][bw][z]++;
				sum_nlbz[l][z]++;
				return;
			}
			System.out.println("bug in sampleBehaviorTopic");
			System.exit(-1);

		} else {
			// sampling based on realm
			// get current realm
			int r = users[u].trainBehaviors[l][b].realms[j];
			// realm-topic
			if (topicRealmSparityFlag) {// p(r|z)
				topicRealmEntropies[currz] = getAdjustedIntElementsEntropy(
						n_zr[currz], topicRealmEntropies[currz],
						sum_nrz[currz], r, -1, "current-behavior-Topic:c=1,zr");
				topicRealmExpParams[currz] = -Math.pow(
						topicRealmEntropies[currz] - topicRealmEntropyMean, 2)
						/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
				sum_nrz[currz]--;
			}
			if (realmTopicSparityFlag) {// p(z|r)
				realmTopicEntropies[r] = getAdjustedIntElementsEntropy(n_rz[r],
						realmTopicEntropies[r], sum_nzr[r], currz, -1,
						"current-behavior-Topic:c=1,rz");
				realmTopicExpParams[r] = -Math.pow(realmTopicEntropyMean
						- realmTopicEntropies[r], 2)
						/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
				n_rz[r][currz]--;
			}
			n_zr[currz][r]--;
			sum_nzr[r]--;
			// topic-behavior
			int bw = users[u].trainBehaviors[l][b].index;
			n_lbz[l][bw][currz]--;
			sum_nlbz[l][currz]--;
			//*************************************************************
			double sump = 0;
			double[] p = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				p[z] = (n_zr[z][r] + eta) / (sum_nzr[r] + sum_eta);
				p[z] = p[z] * (n_lbz[l][bw][z] + gamma)
						/ (sum_nlbz[l][z] + sum_gamma[l]);
				// sparsity condition
				if (topicCoinSparityFlag) {// p(c|z)
					double newZCEntropy = 0;
					if (globalOrPersonalTopicCoinSparityFlag) {// global
						newZCEntropy = getAdjustedIntElementsEntropy(n_zc[z],
								topicCoinEntropies[z], n_zc[z][0] + n_zc[z][1],
								1, 1, "behavior-Topic:zc=1,global");
					} else {// personal
						newZCEntropy = getAdjustedIntElementsEntropy(
								n_uzc[u][z], userTopicCoinEntropies[u][z],
								n_uzc[u][z][0] + n_uzc[u][z][1], 1, 1,
								"behavior-Topic:zc=1,user-specific");
					}
					double zcExpParam = -Math.pow(newZCEntropy
							- topicCoinEntropyMean, 2)
							/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
					if (globalOrPersonalTopicCoinSparityFlag)
						p[z] = p[z]
								* Math.exp(zcExpParam - topicCoinExpParams[z]);
					else
						p[z] = p[z]
								* Math.exp(zcExpParam
										- userTopicCoinExpParams[u][z]);
				}
				if (topicRealmSparityFlag) {// p(r|z)
					double newZREntropy = getAdjustedIntElementsEntropy(
							n_zr[z], topicRealmEntropies[z], sum_nrz[z], r, 1,
							"behavior-Topic:c=1,zr");
					double zrExpParam = -Math.pow(newZREntropy
							- topicRealmEntropyMean, 2)
							/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
					p[z] = p[z] * Math.exp(zrExpParam - topicRealmExpParams[z]);
				}
				if (realmTopicSparityFlag) {// p(z|r)
					double newRZEntropy = getAdjustedIntElementsEntropy(
							n_rz[r], realmTopicEntropies[r], sum_nzr[r], z, 1,
							"behavior-Topic:c=1,rz");
					double rzExpParam = -Math.pow(newRZEntropy
							- realmTopicEntropyMean, 2)
							/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
					p[z] = p[z] * Math.exp(rzExpParam);
				}

				// cumulative
				p[z] = p[z] + sump;
				sump = p[z];
			}
			sump = rand.nextDouble() * sump;
			for (int z = 0; z < nTopics; z++) {
				if (sump > p[z])
					continue;
				// the topic
				users[u].trainBehaviors[l][b].topics[j] = z;
				// user-topic-coin & topic-coin
				if (topicCoinSparityFlag) {// p(c|z)
					if (globalOrPersonalTopicCoinSparityFlag) {// global
						topicCoinEntropies[z] = getAdjustedIntElementsEntropy(
								n_zc[z], topicCoinEntropies[z], n_zc[z][0]
										+ n_zc[z][1], 1, 1,
								"new-behavior-Topic:c=1,zc");
						topicCoinExpParams[z] = -Math.pow(topicCoinEntropies[z]
								- topicCoinEntropyMean, 2)
								/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
						n_zc[z][1]++;
					} else {// personal
						userTopicCoinEntropies[u][z] = getAdjustedIntElementsEntropy(
								n_uzc[u][z], userTopicCoinEntropies[u][z],
								n_uzc[u][z][0] + n_uzc[u][z][1], 1, 1,
								"new-behavior-Topic:c=1,uzc");
						userTopicCoinExpParams[u][z] = -Math.pow(
								userTopicCoinEntropies[u][z]
										- topicCoinEntropyMean, 2)
								/ (2 * topicCoinEntropyVariance * topicCoinEntropyVariance);
						n_uzc[u][z][1]++;
					}
				}
				// realm-topic
				if (topicRealmSparityFlag) {// p(r|z)
					topicRealmEntropies[z] = getAdjustedIntElementsEntropy(
							n_zr[z], topicRealmEntropies[z], sum_nrz[z], r, 1,
							"new-behavior-Topic:c=1,zr");
					topicRealmExpParams[z] = -Math.pow(topicRealmEntropies[z]
							- topicRealmEntropyMean, 2)
							/ (2 * topicRealmEntropyVariance * topicRealmEntropyVariance);
					sum_nrz[z]++;
				}
				if (realmTopicSparityFlag) {// p(z|r)
					realmTopicEntropies[r] = getAdjustedIntElementsEntropy(
							n_rz[r], realmTopicEntropies[r], sum_nzr[r], z, 1,
							"new-behavior-Topic:c=1,rz");
					realmTopicExpParams[r] = -Math.pow(realmTopicEntropies[r]
							- realmTopicEntropyMean, 2)
							/ (2 * realmTopicEntropyVariance * realmTopicEntropyVariance);
					n_rz[r][z]++;
				}
				n_zr[z][r]++;
				sum_nzr[r]++;
				// topic-behavior
				n_lbz[l][bw][z]++;
				sum_nlbz[l][z]++;

				return;
			}
			System.out.println("bug in sampleBehaviorTopic v = 1");
			System.exit(-1);
		}
	}

	private void updateFinalCounts() {
		// user-coin
		for (int u = 0; u < users.length; u++) {
			final_n_cu[0][u] += n_cu[0][u];
			final_n_cu[1][u] += n_cu[1][u];
			final_sum_ncu[u] += sum_ncu[u];
		}
		// user-topic
		for (int u = 0; u < users.length; u++) {
			for (int z = 0; z < nTopics; z++)
				final_n_zu[z][u] += n_zu[z][u];
			final_sum_nzu[u] += sum_nzu[u];
		}
		// user-realm
		for (int u = 0; u < users.length; u++) {
			for (int r = 0; r < nRealms; r++)
				final_n_ru[r][u] += n_ru[r][u];
			final_sum_nru[u] += sum_nru[u];
		}
		// realm-topic
		for (int r = 0; r < nRealms; r++) {
			for (int z = 0; z < nTopics; z++)
				final_n_zr[z][r] += n_zr[z][r];
			final_sum_nzr[r] += sum_nzr[r];
		}
		// topic-word
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				final_n_wz[w][z] += n_wz[w][z];
			final_sum_nwz[z] += sum_nwz[z];
		}
		// topic-behavior
		for (int l = 0; l < nBehaviorTypes; l++) {
			for (int z = 0; z < nTopics; z++) {
				for (int b = 0; b < behaviorVocabularies[l].length; b++)
					final_n_lbz[l][b][z] += n_lbz[l][b][z];
				final_sum_nlbz[l][z] += sum_nlbz[l][z];
			}
		}
		// topic-coin
		if (topicCoinSparityFlag) {
			for (int z = 0; z < nTopics; z++) {
				final_n_zc[z][0] += n_zc[z][0];
				final_n_zc[z][1] += n_zc[z][1];
			}
		}
	}

	private void gibbsSampling() {
		System.out.println("Runing Gibbs sampling");

		System.out.print("Setting prios ...");
		setPriors();
		System.out.println(" Done!");

		declareFinalCounts();

		System.out.print("Initializing ... ");
		initilize();
		System.out.println("... Done!");

		for (int iter = 0; iter <= burningPeriod + maxIteration; iter++) {
			System.out.print("iteration " + iter);

			for (int u = 0; u < users.length; u++) {
				// tweet coin and realm
				for (int t = 0; t < users[u].tweets.length; t++) {
					if (users[u].tweets[t].batch == testBatch)
						continue;
					sampleTweetCoinRealm(u, t);
				}
				// behavior coin and realm
				if (users[u].trainBehaviors == null)
					continue;
				for (int l = 0; l < nBehaviorTypes; l++) {
					if (users[u].trainBehaviors[l] == null)
						continue;
					for (int b = 0; b < users[u].trainBehaviors[l].length; b++) {
						for (int j = 0; j < users[u].trainBehaviors[l][b].count; j++) {
							sampleBehaviorCoinRealm(u, l, b, j);
						}
					}
				}
			}
			for (int u = 0; u < users.length; u++) {
				// tweet topic
				for (int t = 0; t < users[u].tweets.length; t++) {
					if (users[u].tweets[t].batch == testBatch)
						continue;
					sampleTweetTopic(u, t);
				}
				// behavior topic
				if (users[u].trainBehaviors == null)
					continue;
				for (int l = 0; l < nBehaviorTypes; l++) {
					if (users[u].trainBehaviors[l] == null)
						continue;
					for (int b = 0; b < users[u].trainBehaviors[l].length; b++) {
						for (int j = 0; j < users[u].trainBehaviors[l][b].count; j++) {
							sampleBehaviorTopic(u, l, b, j);
						}
					}
				}
			}
			System.out.println(" done!");
			if (samplingGap <= 0)
				continue;
			if (iter < burningPeriod)
				continue;
			if ((iter - burningPeriod) % samplingGap == 0) {
				updateFinalCounts();
			}
		}
		if (samplingGap <= 0)
			updateFinalCounts();
	}

	// inference

	private void inferModelParameters() {
		// user
		for (int u = 0; u < users.length; u++) {
			// realm distribution
			users[u].realmDistribution = new double[nRealms];
			for (int r = 0; r < nRealms; r++) {
				users[u].realmDistribution[r] = (final_n_ru[r][u] + tau)
						/ (final_sum_nru[u] + sum_tau);
			}
			// topic distribution
			users[u].topicDistribution = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				users[u].topicDistribution[z] = (final_n_zu[z][u] + alpha)
						/ (final_sum_nzu[u] + sum_alpha);
			}
			// realm bias
			users[u].realmBias = new double[2];
			users[u].realmBias[0] = (final_n_cu[0][u] + rho)
					/ (final_sum_ncu[u] + sum_rho);
			users[u].realmBias[1] = (final_n_cu[1][u] + rho)
					/ (final_sum_ncu[u] + sum_rho);
		}
		// realm
		realmTopicDistribution = new double[nRealms][nTopics];
		for (int r = 0; r < nRealms; r++) {
			for (int z = 0; z < nTopics; z++) {
				realmTopicDistribution[r][z] = (final_n_zr[z][r] + eta)
						/ (final_sum_nzr[r] + sum_eta);
			}
		}
		// tweet topics
		tweetTopics = new double[nTopics][tweetVocabulary.length];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				tweetTopics[z][w] = (final_n_wz[w][z] + beta)
						/ (final_sum_nwz[z] + sum_beta);
		}
		// behavior topics
		behaviorTopics = new double[nBehaviorTypes][][];
		for (int l = 0; l < nBehaviorTypes; l++) {
			behaviorTopics[l] = new double[nTopics][behaviorVocabularies[l].length];
			for (int z = 0; z < nTopics; z++) {
				for (int bw = 0; bw < behaviorVocabularies[l].length; bw++)
					behaviorTopics[l][z][bw] = (final_n_lbz[l][bw][z] + gamma)
							/ (final_sum_nlbz[l][z] + sum_gamma[l]);
			}
		}
	}

	public void learnModel() {
		gibbsSampling();
		inferModelParameters();
		inferTweetTopicCoinRealmMLE();
		inferBehaviorTopicCoinRealmMLE();
		getLikelihoodPerplexity();
	}

	// evaluating performance and output

	private double getTweetProbability(int u, int t, int z) {
		// compute likelihood of tweet number t of user number u, given topic z
		double p = 1;
		for (int i = 0; i < users[u].tweets[t].words.length; i++) {
			int w = users[u].tweets[t].words[i];
			p = p * tweetTopics[z][w];
		}
		return p;
	}

	private double getTweetLikelihood(int u, int t) {
		// compute likelihood of tweet number t of user number u
		double pv_0 = 0;
		double pv_1 = 0;

		for (int z = 0; z < nTopics; z++) {
			double p_z = 1;
			for (int i = 0; i < users[u].tweets[t].words.length; i++) {
				int w = users[u].tweets[t].words[i];
				p_z = p_z * tweetTopics[z][w];
			}
			pv_0 += p_z * users[u].topicDistribution[z];

			double pRealm = 0;
			for (int r = 0; r < nRealms; r++) {
				pRealm = pRealm + realmTopicDistribution[r][z]
						* users[u].realmDistribution[r];
			}
			pv_1 += p_z * pRealm;
		}
		pv_0 = pv_0 * users[u].realmBias[0];
		pv_1 = pv_1 * users[u].realmBias[1];

		double logLikelihood = Math.log10(pv_0 + pv_1);
		return logLikelihood;
	}

	private double getBehaviorLikelihood(int u, int l, int b, boolean flag) {
		// compute likelihood of the type-l behavior at index b
		// flag to indicate whether the behavior is in training set (flag ==
		// true) or test set (flag == false)
		int bw;
		if (flag)
			bw = users[u].trainBehaviors[l][b].index;
		else
			bw = users[u].testBehaviors[l][b].index;

		double pv_0 = 0;
		double pv_1 = 0;

		for (int z = 0; z < nTopics; z++) {
			pv_0 += behaviorTopics[l][z][bw] * users[u].topicDistribution[z];

			double pRealm = 0;
			for (int r = 0; r < nRealms; r++) {
				pRealm = pRealm + realmTopicDistribution[r][z]
						* users[u].realmDistribution[r];
			}
			pv_1 += behaviorTopics[l][z][bw] * pRealm;
		}

		pv_0 = pv_0 * users[u].realmBias[0];
		pv_1 = pv_1 * users[u].realmBias[1];
		double logLikelihood = Math.log10(pv_0 + pv_1);
		return logLikelihood;
	}

	private void getLikelihoodPerplexity() {
		tweetLogLikelidhood = 0;
		tweetLogPerplexity = 0;
		behaviorLogLikelidhoods = new double[nBehaviorTypes];
		behaviorLogPerplexities = new double[nBehaviorTypes];
		int nTestTweet = 0;
		int[] nTestBehavior = new int[nBehaviorTypes];
		for (int l = 0; l < nBehaviorTypes; l++) {
			behaviorLogLikelidhoods[l] = 0;
			behaviorLogPerplexities[l] = 0;
			nTestBehavior[l] = 0;
		}
		for (int u = 0; u < users.length; u++) {
			// tweet
			for (int t = 0; t < users[u].tweets.length; t++) {
				double logLikelihood = getTweetLikelihood(u, t);
				if (users[u].tweets[t].batch != testBatch)
					tweetLogLikelidhood += logLikelihood;
				else {
					tweetLogPerplexity += (-logLikelihood);
					nTestTweet++;
				}
			}
			// behavior
			if (users[u].trainBehaviors == null)
				continue;
			for (int l = 0; l < nBehaviorTypes; l++) {
				if (users[u].trainBehaviors[l] == null)
					continue;
				for (int b = 0; b < users[u].trainBehaviors[l].length; b++) {
					double logLikelihood = getBehaviorLikelihood(u, l, b, true);
					behaviorLogLikelidhoods[l] += logLikelihood
							* users[u].trainBehaviors[l][b].count;
				}
				if (users[u].testBehaviors[l] == null)
					continue;
				for (int b = 0; b < users[u].testBehaviors[l].length; b++) {
					double logLikelihood = getBehaviorLikelihood(u, l, b, false);
					behaviorLogPerplexities[l] += (-logLikelihood)
							* users[u].testBehaviors[l][b].count;
					nTestBehavior[l] += users[u].testBehaviors[l][b].count;
				}
			}
		}
		tweetLogPerplexity /= nTestTweet;
		for (int l = 0; l < nBehaviorTypes; l++) {
			behaviorLogPerplexities[l] /= nTestBehavior[l];
		}

	}

	private void outputTweetTopics() {
		try {
			String fileName = outputPath + SystemTool.pathSeparator
					+ "tweetTopics.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			for (int z = 0; z < nTopics; z++) {
				bw.write("" + z);
				for (int w = 0; w < tweetVocabulary.length; w++)
					bw.write("," + tweetTopics[z][w]);
				bw.write("\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out tweet topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTweetTopicTopWords(int k) {
		try {
			String fileName = outputPath + SystemTool.pathSeparator
					+ "tweetTopicTopWords.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			for (int z = 0; z < nTopics; z++) {
				bw.write(z + "\n");
				RankingTool rankTool = new RankingTool();
				WeightedElement[] topWords = rankTool.getTopKbyWeight(
						tweetVocabulary, tweetTopics[z], k);
				for (int j = 0; j < k; j++)
					bw.write("," + topWords[j].name + "," + topWords[j].weight
							+ "\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void inferTweetTopicCoinRealmMLE() {
		for (int u = 0; u < users.length; u++) {
			for (int t = 0; t < users[u].tweets.length; t++) {
				double[] tweetProbs = new double[nTopics];
				for (int z = 0; z < nTopics; z++)
					tweetProbs[z] = getTweetProbability(u, t, z);
				// when coin = 0
				int z_0 = -1;
				double maxp_0 = -1;
				for (int z = 0; z < nTopics; z++) {
					double pz = tweetProbs[z] * users[u].topicDistribution[z]
							* users[u].realmBias[0];
					if (pz > maxp_0) {
						maxp_0 = pz;
						z_0 = z;
					}
				}
				// when coin = 1;
				int rmax = -1;
				int z_1 = -1;
				double maxp_1 = -1;
				for (int r = 0; r < nRealms; r++) {
					for (int z = 0; z < nTopics; z++) {
						double pz = tweetProbs[z]
								* realmTopicDistribution[r][z]
								* users[u].realmDistribution[r]
								* users[u].realmBias[1];
						if (pz > maxp_1) {
							maxp_1 = pz;
							z_1 = z;
							rmax = r;
						}
					}
				}

				if (maxp_0 > maxp_1) {
					users[u].tweets[t].inferedCoin = 0;
					users[u].tweets[t].inferedRealm = -1;
					users[u].tweets[t].inferedTopic = z_0;
					users[u].tweets[t].inferedLikelihood = maxp_0;
				} else {
					users[u].tweets[t].inferedCoin = 1;
					users[u].tweets[t].inferedRealm = rmax;
					users[u].tweets[t].inferedTopic = z_1;
					users[u].tweets[t].inferedLikelihood = maxp_1;
				}

			}
		}
	}

	private void outputTweetTopicTopTweets(int k) {
		int[] tweetPerTopicCount = new int[nTopics];
		for (int z = 0; z < nTopics; z++)
			tweetPerTopicCount[z] = 0;
		for (int u = 0; u < users.length; u++) {
			for (int t = 0; t < users[u].tweets.length; t++) {
				if (users[u].tweets[t].batch == testBatch)
					continue;
				tweetPerTopicCount[users[u].tweets[t].inferedTopic]++;
			}
		}

		String[][] tweetID = new String[nTopics][];
		double[][] perTweetPerplexity = new double[nTopics][];
		for (int z = 0; z < nTopics; z++) {
			tweetID[z] = new String[tweetPerTopicCount[z]];
			perTweetPerplexity[z] = new double[tweetPerTopicCount[z]];
			tweetPerTopicCount[z] = 0;
		}

		for (int u = 0; u < users.length; u++) {
			for (int t = 0; t < users[u].tweets.length; t++) {
				if (users[u].tweets[t].batch == testBatch)
					continue;
				int z = users[u].tweets[t].inferedTopic;
				tweetID[z][tweetPerTopicCount[z]] = users[u].tweets[t].tweetID;
				perTweetPerplexity[z][tweetPerTopicCount[z]] = users[u].tweets[t].inferedLikelihood
						/ users[u].tweets[t].words.length;
				tweetPerTopicCount[z]++;
			}
		}

		try {
			String fileName = outputPath + SystemTool.pathSeparator
					+ "tweetTopicTopTweets.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));

			for (int z = 0; z < nTopics; z++) {
				bw.write(z + "\n");
				RankingTool rankTool = new RankingTool();
				WeightedElement[] topTweets = rankTool.getTopKbyWeight(
						tweetID[z], perTweetPerplexity[z], k);
				for (int j = 0; j < k; j++)
					bw.write("," + topTweets[j].name + ","
							+ topTweets[j].weight + "\n");
			}

			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out tweet topic top tweets to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void inferBehaviorTopicCoinRealmMLE() {
		for (int u = 0; u < users.length; u++) {
			if (users[u].trainBehaviors == null)
				continue;
			for (int l = 0; l < nBehaviorTypes; l++) {
				if (users[u].trainBehaviors[l] == null)
					continue;
				for (int b = 0; b < users[u].trainBehaviors[l].length; b++) {
					int j = users[u].trainBehaviors[l][b].index;
					// when coin = 0
					int z_0 = -1;
					double maxp_0 = -1;
					for (int z = 0; z < nTopics; z++) {
						double pz = behaviorTopics[l][z][j]
								* users[u].topicDistribution[z]
								* users[u].realmBias[0];
						if (pz > maxp_0) {
							maxp_0 = pz;
							z_0 = z;
						}
					}
					// when coin = 1;
					int rmax = -1;
					int z_1 = -1;
					double maxp_1 = -1;
					for (int r = 0; r < nRealms; r++) {
						for (int z = 0; z < nTopics; z++) {
							double pz = behaviorTopics[l][z][j]
									* realmTopicDistribution[r][z]
									* users[u].realmDistribution[r]
									* users[u].realmBias[1];
							if (pz > maxp_1) {
								maxp_1 = pz;
								z_1 = z;
								rmax = r;
							}
						}
					}

					if (maxp_0 > maxp_1) {
						users[u].trainBehaviors[l][b].inferedCoin = 0;
						users[u].trainBehaviors[l][b].inferedRealm = -1;
						users[u].trainBehaviors[l][b].inferedTopic = z_0;
						users[u].trainBehaviors[l][b].inferedLikelihood = maxp_0;
					} else {
						users[u].trainBehaviors[l][b].inferedCoin = 1;
						users[u].trainBehaviors[l][b].inferedRealm = rmax;
						users[u].trainBehaviors[l][b].inferedTopic = z_1;
						users[u].trainBehaviors[l][b].inferedLikelihood = maxp_1;
					}
				}
			}
		}
	}

	private void outputBehaviorTopics() {
		try {
			for (int i = 0; i < nBehaviorTypes; i++) {
				String fileName = outputPath + SystemTool.pathSeparator
						+ "behavior_" + behaviorNames[i] + ".csv";
				File file = new File(fileName);
				if (!file.exists()) {
					file.createNewFile();
				}
				BufferedWriter bw = new BufferedWriter(new FileWriter(
						file.getAbsoluteFile()));
				for (int z = 0; z < nTopics; z++) {
					bw.write("" + z);
					for (int w = 0; w < behaviorVocabularies[i].length; w++)
						bw.write("," + behaviorTopics[i][z][w]);
					bw.write("\n");
				}
				bw.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing out behavior topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputBehaviorTopicTopWords(int k) {
		try {
			for (int i = 0; i < nBehaviorTypes; i++) {
				String fileName = outputPath + SystemTool.pathSeparator
						+ behaviorNames[i] + "TopicTopWords.csv";
				File file = new File(fileName);
				if (!file.exists()) {
					file.createNewFile();
				}
				BufferedWriter bw = new BufferedWriter(new FileWriter(
						file.getAbsoluteFile()));
				for (int z = 0; z < nTopics; z++) {
					bw.write(z + "\n");
					RankingTool rankTool = new RankingTool();
					WeightedElement[] topWords = rankTool.getTopKbyWeight(
							behaviorVocabularies[i], behaviorTopics[i][z], k);
					for (int j = 0; j < k; j++)
						bw.write("," + topWords[j].name + ","
								+ topWords[j].weight + "\n");
				}
				bw.close();
			}
		} catch (Exception e) {
			System.out
					.println("Error in writing out behavior topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputUserTopicDistribution() {
		try {
			String fileName = outputPath + SystemTool.pathSeparator
					+ "userTopics.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			for (int u = 0; u < users.length; u++) {
				bw.write("" + users[u].userID);
				for (int z = 0; z < nTopics; z++)
					bw.write("," + users[u].topicDistribution[z]);
				bw.write("\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out user topic distributions to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputUserBiasRealmDistribution() {
		try {
			String fileName = outputPath + SystemTool.pathSeparator
					+ "userRealm.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			bw.write("userID,conformity_0, conformity_1");
			for (int r = 0; r < nRealms; r++)
				bw.write(",realm_" + r);
			bw.write("\n");

			for (int u = 0; u < users.length; u++) {
				bw.write("" + users[u].userID + "," + users[u].realmBias[0]
						+ "," + users[u].realmBias[1]);
				for (int r = 0; r < nRealms; r++)
					bw.write("," + users[u].realmDistribution[r]);
				bw.write("\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out user conformmity and realm distributions to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputRealmTopicDistribution() {
		try {

			String fileName = outputPath + SystemTool.pathSeparator
					+ "realmTopicsDistribution.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			for (int r = 0; r < nRealms; r++) {
				bw.write("" + r);
				for (int z = 0; z < nTopics; z++)
					bw.write("," + realmTopicDistribution[r][z]);
				bw.write("\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out realm topic distributions to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputRealmTopicCount() {
		try {

			String fileName = outputPath + SystemTool.pathSeparator
					+ "realmTopicsCount.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			for (int r = 0; r < nRealms; r++) {
				bw.write("" + r);
				for (int z = 0; z < nTopics; z++)
					bw.write("," + final_n_zr[z][r]);
				bw.write("\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out realm topic counts to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputCoinTopicCount() {
		try {

			String fileName = outputPath + SystemTool.pathSeparator
					+ "coinTopicsCount.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			for (int z = 0; z < nTopics; z++)
				bw.write(final_n_zc[z][0] + "," + final_n_zc[z][1] + "\n");

			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out coin topic counts to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputLikelihoodPerplexity() {
		try {
			String fileName = outputPath + SystemTool.pathSeparator
					+ "likelihood-perplexity.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			bw.write("tweetLogLikelihood,tweetLogPerplexity");
			for (int l = 0; l < nBehaviorTypes; l++)
				bw.write("," + behaviorNames[l] + "LogLikelihood,"
						+ behaviorNames[l] + "LogPerplexity");
			bw.write("\n");
			bw.write("" + tweetLogLikelidhood + "," + tweetLogPerplexity);
			for (int i = 0; i < nBehaviorTypes; i++)
				bw.write("," + behaviorLogLikelidhoods[i] + ","
						+ behaviorLogPerplexities[i]);
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputInferedTopicCoinCount() {
		try {
			double[] personalCount = new double[nTopics];
			double[] commCount = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				personalCount[z] = 0;
				commCount[z] = 0;
			}

			for (int u = 0; u < users.length; u++) {
				for (int t = 0; t < users[u].tweets.length; t++)
					if (users[u].tweets[t].inferedCoin == 0)
						personalCount[users[u].tweets[t].inferedTopic]++;
					else
						commCount[users[u].tweets[t].inferedTopic]++;

				if (users[u].trainBehaviors == null)
					continue;
				for (int l = 0; l < nBehaviorTypes; l++) {
					if (users[u].trainBehaviors[l] == null)
						continue;
					for (int b = 0; b < users[u].trainBehaviors[l].length; b++) {
						if (users[u].trainBehaviors[l][b].inferedCoin == 0)
							personalCount[users[u].trainBehaviors[l][b].inferedTopic] += users[u].trainBehaviors[l][b].count;
						else
							commCount[users[u].trainBehaviors[l][b].inferedTopic] += users[u].trainBehaviors[l][b].count;
					}
				}
			}
			String fileName = outputPath + SystemTool.pathSeparator
					+ "inferedTopicCoinCount.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));

			bw.write("personal," + personalCount[0]);
			for (int z = 1; z < nTopics; z++)
				bw.write("," + personalCount[z]);
			bw.write("\n");
			bw.write("realm," + commCount[0]);
			for (int z = 1; z < nTopics; z++)
				bw.write("," + commCount[z]);
			bw.close();

		} catch (Exception e) {
			System.out.println("Error in writing out tweet topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputLearningOptions() {
		try {
			String fileName = outputPath + SystemTool.pathSeparator
					+ "learningOptions.txt";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			bw.write("burningPeriod = " + burningPeriod + "\n");
			bw.write("maxIteration = " + maxIteration + "\n");
			bw.write("samplingGap = " + samplingGap + "\n");
			bw.write("testBatch = " + testBatch + "\n");

			bw.write("topicCoinSparityFlag = " + topicCoinSparityFlag + "\n");
			bw.write("globalOrPersonalTopicCoinSparityFlag = "
					+ globalOrPersonalTopicCoinSparityFlag + "\n");
			bw.write("topicRealmSparityFlag = " + topicRealmSparityFlag + "\n");
			bw.write("realmTopicSparityFlag = " + realmTopicSparityFlag + "\n");

			bw.write("topicCoinEntropyMean = " + topicCoinEntropyMean + "\n");
			bw.write("topicCoinEntropyVariance = " + topicCoinEntropyVariance
					+ "\n");

			bw.write("topicRealmEntropyMean = " + topicRealmEntropyMean + "\n");
			bw.write("topicRealmEntropyVariance = " + topicRealmEntropyVariance
					+ "\n");

			bw.write("realmTopicEntropyMean = " + realmTopicEntropyMean + "\n");
			bw.write("realmTopicEntropyVariance = " + realmTopicEntropyVariance
					+ "\n");

			bw.close();

		} catch (Exception e) {
			System.out
					.println("Error in writing out learning options to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void outputLearntParams() {
		outputLearningOptions();
		outputTweetTopics();
		outputTweetTopicTopWords(20);
		outputTweetTopicTopTweets(50);
		outputBehaviorTopics();
		outputBehaviorTopicTopWords(20);
		outputUserTopicDistribution();
		outputUserBiasRealmDistribution();
		outputRealmTopicDistribution();
		outputRealmTopicCount();
		if (topicCoinSparityFlag) {
			outputCoinTopicCount();
		}
		outputLikelihoodPerplexity();
		outputInferedTopicCoinCount();
	}
}
