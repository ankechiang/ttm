//TwitterLDA with background topic
package hoang.topicmodel.model.twitterLDA;

import hoang.larc.tooler.RankingTool;
import hoang.larc.tooler.SystemTool;
import hoang.larc.tooler.WeightedElement;
import hoang.topicmodel.data.Tweet;
import hoang.topicmodel.data.User;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.util.HashMap;
import java.util.Random;

import org.apache.commons.io.FilenameUtils;

public class BTwitterLDA {
	//
	public String dataPath;
	public String outputPath;
	public int nTopics;

	public int burningPeriod;
	public int maxIteration;
	public int samplingGap;
	public int testBatch;

	public Random rand;
	// hyperparameters
	private double alpha;
	private double sum_alpha;
	private double beta;
	private double sum_beta;
	private double[] gamma;
	private double sum_gamma;
	// data
	private User[] users;

	private String[] tweetVocabulary;
	// parameters
	private double[][] tweetTopics;
	private double[] backgroundTopic;
	private double[] coinBias;

	// Gibbs sampling variables
	// user - topic count
	private int[][] n_zu; // n_zu[k,u]: number of times topic z is observed in
							// tweets by user u
	private int[] sum_nzu;// sum_nzu[u]: total number of topics that are
							// observed in tweets by user u
	// topic - word count
	private int[][] n_wz;// n_wz[w,z]: number of times word w is generated by a
							// topic z in all tweets
	private int[] sum_nwz;// sum_nw[z]: total number of words that are generated
							// by a topic z in tweets
	private int[] n_wb;// n_wz[w]: number of times word w is generated by a
						// background topic
	private int sum_nwb;// sum_nw[z]: total number of words that are generated
						// by background topic

	// topic - coin count
	private int[] n_c;// sum_nw[c]: total number of words that are
						// associated with coin c
	private int sum_nc;

	private int[][] final_n_zu;
	private int[] final_sum_nzu;
	private int[][] final_n_wz;
	private int[] final_sum_nwz;
	private int[] final_n_wb;
	private int final_sum_nwb;
	private int[] final_n_c;
	private int final_sum_nc;

	private double tweetLogLikelidhood;
	private double tweetLogPerplexity;

	public void readData() {
		BufferedReader br = null;
		String line = null;
		HashMap<String, Integer> userId2Index = null;
		HashMap<Integer, String> userIndex2Id = null;
		// read tweet
		try {
			String folderName = dataPath + "/tweet/users";
			File tweetFolder = new File(folderName);
			// read number of users
			int nUser = tweetFolder.listFiles().length;
			users = new User[nUser];

			userId2Index = new HashMap<String, Integer>(nUser);
			userIndex2Id = new HashMap<Integer, String>(nUser);
			int u = -1;
			for (File tweetFile : tweetFolder.listFiles()) {
				u++;
				users[u] = new User();
				// index of the user
				String userId = FilenameUtils.removeExtension(tweetFile
						.getName());
				userId2Index.put(userId, u);
				userIndex2Id.put(u, userId);
				users[u].userID = userId;
				// read the tweet
				int nTweet = 0;
				br = new BufferedReader(new FileReader(
						tweetFile.getAbsolutePath()));
				while (br.readLine() != null) {
					nTweet++;
				}
				br.close();

				users[u].tweets = new Tweet[nTweet];

				br = new BufferedReader(new FileReader(
						tweetFile.getAbsolutePath()));
				int j = -1;
				while ((line = br.readLine()) != null) {
					j++;
					users[u].tweets[j] = new Tweet();
					String[] tokens = line.split(" ");
					users[u].tweets[j].tweetID = tokens[0];
					users[u].tweets[j].batch = Integer.parseInt(tokens[1]);
					users[u].tweets[j].words = new int[tokens.length - 2];
					for (int i = 0; i < tokens.length - 2; i++)
						users[u].tweets[j].words[i] = Integer
								.parseInt(tokens[i + 2]);
				}
				br.close();
			}

			// read tweet vocabulary
			String tweetVocabularyFileName = dataPath + "/tweet/vocabulary.txt";

			br = new BufferedReader(new FileReader(tweetVocabularyFileName));
			int nTweetWord = 0;
			while (br.readLine() != null) {
				nTweetWord++;
			}
			br.close();
			tweetVocabulary = new String[nTweetWord];

			br = new BufferedReader(new FileReader(tweetVocabularyFileName));
			while ((line = br.readLine()) != null) {
				String[] tokens = line.split(",");
				int index = Integer.parseInt(tokens[0]);
				tweetVocabulary[index] = tokens[1];
			}
			br.close();

		} catch (Exception e) {
			System.out.println("Error in reading tweet from file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void declareFinalCounts() {
		final_n_zu = new int[nTopics][users.length];
		final_sum_nzu = new int[users.length];
		for (int u = 0; u < users.length; u++) {
			for (int z = 0; z < nTopics; z++)
				final_n_zu[z][u] = 0;
			final_sum_nzu[u] = 0;
		}
		final_n_wz = new int[tweetVocabulary.length][nTopics];
		final_sum_nwz = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				final_n_wz[w][z] = 0;
			final_sum_nwz[z] = 0;
		}

		final_n_wb = new int[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			final_n_wb[w] = 0;
		final_sum_nwb = 0;

		final_n_c = new int[2];
		for (int c = 0; c < 2; c++) {
			final_n_c[c] = 0;
		}
		final_sum_nc = 0;
	}

	private void initilize() {
		// init coin and topic for each tweet and each behavior
		for (int u = 0; u < users.length; u++) {
			// tweet
			for (int t = 0; t < users[u].tweets.length; t++) {
				if (users[u].tweets[t].batch == testBatch)
					continue;
				users[u].tweets[t].topic = rand.nextInt(nTopics);
				int nWords = users[u].tweets[t].words.length;
				users[u].tweets[t].coins = new int[nWords];
				for (int i = 0; i < users[u].tweets[t].coins.length; i++)
					users[u].tweets[t].coins[i] = rand.nextInt(2);
			}
		}
		// declare and initiate counting tables
		n_zu = new int[nTopics][users.length];
		sum_nzu = new int[users.length];
		for (int u = 0; u < users.length; u++) {
			for (int z = 0; z < nTopics; z++)
				n_zu[z][u] = 0;
			sum_nzu[u] = 0;
		}
		n_wz = new int[tweetVocabulary.length][nTopics];
		sum_nwz = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				n_wz[w][z] = 0;
			sum_nwz[z] = 0;
		}
		n_wb = new int[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			n_wb[w] = 0;
		sum_nwb = 0;

		n_c = new int[2];
		n_c[0] = 0;
		n_c[1] = 0;
		sum_nc = 0;
		// update counting tables
		for (int u = 0; u < users.length; u++) {
			// tweet
			for (int t = 0; t < users[u].tweets.length; t++) {
				if (users[u].tweets[t].batch == testBatch)
					continue;
				int z = users[u].tweets[t].topic;
				// user-topic and community-topic
				n_zu[z][u]++;
				sum_nzu[u]++;
				for (int i = 0; i < users[u].tweets[t].words.length; i++) {
					int w = users[u].tweets[t].words[i];
					int c = users[u].tweets[t].coins[i];
					// coin count
					n_c[c]++;
					sum_nc++;
					if (c == 0) {
						// word - background topic
						n_wb[w]++;
						sum_nwb++;
					} else {
						// word - topic
						n_wz[w][z]++;
						sum_nwz[z]++;
					}

				}
			}
		}
	}

	// sampling
	private void setPriors() {
		// user topic prior
		alpha = 50.0 / nTopics;
		sum_alpha = 50;

		// topic tweet word prior
		beta = 0.01;
		sum_beta = 0.01 * tweetVocabulary.length;
		// biased coin prior
		gamma = new double[2];
		gamma[0] = 2;
		gamma[1] = 2;
		sum_gamma = gamma[0] + gamma[1];
	}

	private void sampleTweetTopic(int u, int t) {
		// sample the topic for tweet number t of user number u
		// get current topic
		int currz = users[u].tweets[t].topic;
		// sampling based on user interest
		n_zu[currz][u]--;
		sum_nzu[u]--;
		for (int i = 0; i < users[u].tweets[t].words.length; i++) {
			if (users[u].tweets[t].coins[i] == 0)
				continue;// do not consider background words
			int w = users[u].tweets[t].words[i];
			n_wz[w][currz]--;
			sum_nwz[currz]--;
		}
		double sump = 0;
		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			p[z] = (n_zu[z][u] + alpha) / (sum_nzu[u] + sum_alpha);
			for (int i = 0; i < users[u].tweets[t].words.length; i++) {
				if (users[u].tweets[t].coins[i] == 0)
					continue;// do not consider background words
				int w = users[u].tweets[t].words[i];
				p[z] = p[z] * (n_wz[w][z] + beta) / (sum_nwz[z] + sum_beta);
			}
			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}
		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z])
				continue;
			// the topic
			users[u].tweets[t].topic = z;
			// user - topic
			n_zu[z][u]++;
			sum_nzu[u]++;
			// topic - word
			for (int i = 0; i < users[u].tweets[t].words.length; i++) {
				if (users[u].tweets[t].coins[i] == 0)
					continue;// do not consider background words
				int w = users[u].tweets[t].words[i];
				n_wz[w][z]++;
				sum_nwz[z]++;
			}
			return;
		}
		System.out.println("bug in sampleTweetTopic");
		for (int z = 0; z < nTopics; z++) {
			System.out.print(p[z] + " ");
		}
		System.exit(-1);
	}

	private void sampleWordCoin(int u, int t, int i) {
		// sample the coin for the word number i of the tweet number t of user
		// number u
		// get current coin
		int currc = users[u].tweets[t].coins[i];
		// get current word
		int w = users[u].tweets[t].words[i];
		// get current topic
		int z = users[u].tweets[t].topic;
		// coin count
		n_c[currc]--;
		sum_nc--;
		if (currc == 0) {
			// word - background topic
			n_wb[w]--;
			sum_nwb--;
		} else {
			// word - topic
			n_wz[w][z]--;
			sum_nwz[z]--;
		}

		// probability of coin 0 given priors and recent counts
		double p_0 = (n_c[0] + gamma[0]) / (sum_nc + sum_gamma);
		// probability of w given coin 0
		p_0 = p_0 * (n_wb[w] + beta) / (sum_nwb + sum_beta);

		// probability of coin 1 given priors and recent counts
		double p_1 = (n_c[1] + gamma[1]) / (sum_nc + sum_gamma);
		// probability of w given coin 1 and topic z
		p_1 = p_1 * (n_wz[w][z] + beta) / (sum_nwz[z] + sum_beta);

		double sump = p_0 + p_1;
		sump = rand.nextDouble() * sump;
		int c = 0;
		if (sump > p_0)
			c = 1;
		// the coin
		users[u].tweets[t].coins[i] = c;
		// coin count
		n_c[c]++;
		sum_nc++;
		if (c == 0) {
			// word-background topic
			n_wb[w]++;
			sum_nwb++;
		} else {
			// word - topic
			n_wz[w][z]++;
			sum_nwz[z]++;
		}
	}

	private void updateFinalCounts() {
		for (int u = 0; u < users.length; u++) {
			for (int z = 0; z < nTopics; z++)
				final_n_zu[z][u] += n_zu[z][u];
			final_sum_nzu[u] += sum_nzu[u];
		}

		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				final_n_wz[w][z] += n_wz[w][z];
			final_sum_nwz[z] += sum_nwz[z];
		}

		for (int w = 0; w < tweetVocabulary.length; w++)
			final_n_wb[w] += n_wb[w];
		final_sum_nwb += sum_nwb;

		for (int c = 0; c < 2; c++) {
			final_n_c[c] += n_c[c];
		}
		final_sum_nc += sum_nc;
	}

	private void gibbsSampling() {
		System.out.println("Runing Gibbs sampling");
		System.out.print("Setting prios ...");
		setPriors();
		System.out.println(" Done!");
		declareFinalCounts();
		System.out.print("Initializing ... ");
		initilize();
		System.out.println("... Done!");
		for (int iter = 0; iter < burningPeriod + maxIteration; iter++) {
			System.out.print("iteration " + iter);
			// topic
			for (int u = 0; u < users.length; u++) {
				for (int t = 0; t < users[u].tweets.length; t++) {
					if (users[u].tweets[t].batch == testBatch)
						continue;
					sampleTweetTopic(u, t);
				}
			}
			// coin
			for (int u = 0; u < users.length; u++) {
				for (int t = 0; t < users[u].tweets.length; t++) {
					if (users[u].tweets[t].batch == testBatch)
						continue;
					for (int i = 0; i < users[u].tweets[t].words.length; i++)
						sampleWordCoin(u, t, i);
				}
			}

			System.out.println(" done!");
			if (samplingGap <= 0)
				continue;
			if (iter < burningPeriod)
				continue;
			if ((iter - burningPeriod) % samplingGap == 0) {
				updateFinalCounts();
			}
		}
		if (samplingGap <= 0)
			updateFinalCounts();
	}

	// inference
	private void inferingModelParameters() {
		// user
		for (int u = 0; u < users.length; u++) {
			// topic distribution
			users[u].topicDistribution = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				users[u].topicDistribution[z] = (final_n_zu[z][u] + alpha)
						/ (final_sum_nzu[u] + sum_alpha);
			}
		}
		// topics
		tweetTopics = new double[nTopics][tweetVocabulary.length];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				tweetTopics[z][w] = (final_n_wz[w][z] + beta)
						/ (final_sum_nwz[z] + sum_beta);
		}

		// background topics
		backgroundTopic = new double[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			backgroundTopic[w] = (final_n_wb[w] + beta)
					/ (final_sum_nwb + sum_beta);
		// coin bias
		coinBias = new double[2];
		coinBias[0] = (final_n_c[0] + gamma[0]) / (final_sum_nc + sum_gamma);
		coinBias[1] = (final_n_c[1] + gamma[1]) / (final_sum_nc + sum_gamma);

	}

	public void learnModel() {
		gibbsSampling();
		inferingModelParameters();
		inferTweetTopic();
		getLikelihoodPerplexity();
	}

	private double getTweetLikelihood(int u, int t) {
		// compute likelihood of tweet number t of user number u
		double logLikelihood = 0;
		for (int i = 0; i < users[u].tweets[t].words.length; i++) {
			int w = users[u].tweets[t].words[i];
			// probability that word i is generated by background topic
			double p_0 = backgroundTopic[w] * coinBias[0];
			// probability that word i is generated by other topics
			double p_1 = 0;
			for (int z = 0; z < nTopics; z++) {
				double p_z = tweetTopics[z][w] * users[u].topicDistribution[z];
				p_1 = p_1 + p_z;
			}
			p_1 = p_1 * coinBias[1];

			logLikelihood = logLikelihood + Math.log10(p_0 + p_1);
			/*
			 * if (Double.isNaN(logLikelihood)) { System.out.println("p_0 = " +
			 * p_0 + "\tp_1 = " + p_1); // System.exit(-1); }
			 */
		}

		return logLikelihood;
	}

	private double getTweetLikelihood(int u, int t, int z) {
		// compute likelihood of tweet number t of user number u given the topic
		// z
		if (z >= 0) {
			double logLikelihood = 0;
			for (int i = 0; i < users[u].tweets[t].words.length; i++) {
				int w = users[u].tweets[t].words[i];
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w] * coinBias[0];
				// probability that word i is generated by topic z
				double p_1 = tweetTopics[z][w] * coinBias[1];
				logLikelihood = logLikelihood + Math.log10(p_0 + p_1);
			}
			return logLikelihood;
		} else {
			double logLikelihood = 0;
			for (int i = 0; i < users[u].tweets[t].words.length; i++) {
				int w = users[u].tweets[t].words[i];
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w];
				logLikelihood = logLikelihood + Math.log10(p_0);
			}
			return logLikelihood;
		}
	}

	private void getLikelihoodPerplexity() {
		tweetLogLikelidhood = 0;
		tweetLogPerplexity = 0;
		int nTestTweet = 0;
		for (int u = 0; u < users.length; u++) {
			// tweet
			for (int t = 0; t < users[u].tweets.length; t++) {
				double logLikelihood = getTweetLikelihood(u, t);
				if (users[u].tweets[t].batch != testBatch)
					tweetLogLikelidhood += logLikelihood;
				else {
					tweetLogPerplexity += (-logLikelihood);
					nTestTweet++;
				}
			}
		}
		tweetLogPerplexity /= nTestTweet;
	}

	private void inferTweetTopic() {
		for (int u = 0; u < users.length; u++) {
			for (int t = 0; t < users[u].tweets.length; t++) {
				users[u].tweets[t].inferedTopic = -1;// background topic only
				users[u].tweets[t].inferedLikelihood = users[u].tweets.length
						* Math.log10(coinBias[0])
						+ getTweetLikelihood(u, t, -1);

				for (int z = 0; z < nTopics; z++) {
					double p_z = getTweetLikelihood(u, t, z);
					p_z += Math.log10(users[u].topicDistribution[z]);

					if (users[u].tweets[t].inferedLikelihood < p_z) {
						users[u].tweets[t].inferedLikelihood = p_z;
						users[u].tweets[t].inferedTopic = z;
					}
				}
			}
		}
	}

	private void outputTweetTopics() {
		try {
			String fileName = outputPath + "/tweetTopics.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			for (int z = 0; z < nTopics; z++) {
				bw.write("" + z);
				for (int w = 0; w < tweetVocabulary.length; w++)
					bw.write("," + tweetTopics[z][w]);
				bw.write("\n");
			}
			bw.write("background");
			for (int w = 0; w < tweetVocabulary.length; w++)
				bw.write("," + backgroundTopic[w]);
			bw.write("\n");
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputCoinBias() {
		try {
			String fileName = outputPath + "/coinBias.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			bw.write(coinBias[0] + "," + coinBias[1]);
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out coin bias to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTweetTopicTopWords(int k) {
		try {
			String fileName = outputPath + "/tweetTopicTopWords.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			RankingTool rankTool = new RankingTool();
			WeightedElement[] topWords = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(z + "\n");
				topWords = rankTool.getTopKbyWeight(tweetVocabulary,
						tweetTopics[z], k);
				for (int j = 0; j < k; j++)
					bw.write("," + topWords[j].name + "," + topWords[j].weight
							+ "\n");
			}

			bw.write("background\n");
			topWords = rankTool.getTopKbyWeight(tweetVocabulary,
					backgroundTopic, 2 * k);
			for (int j = 0; j < 2 * k; j++)
				bw.write("," + topWords[j].name + "," + topWords[j].weight
						+ "\n");

			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out tweet topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTweetTopicTopTweets(int k) {
		int[] tweetPerTopicCount = new int[nTopics];
		int tweetBackgroundTopicCount = 0;
		for (int z = 0; z < nTopics; z++)
			tweetPerTopicCount[z] = 0;
		for (int u = 0; u < users.length; u++) {
			for (int t = 0; t < users[u].tweets.length; t++) {
				if (users[u].tweets[t].batch == testBatch)
					continue;
				if (users[u].tweets[t].inferedTopic >= 0)
					tweetPerTopicCount[users[u].tweets[t].inferedTopic]++;
				else
					tweetBackgroundTopicCount++;
			}
		}

		String[][] tweetID = new String[nTopics][];
		double[][] perTweetPerplexity = new double[nTopics][];
		for (int z = 0; z < nTopics; z++) {
			tweetID[z] = new String[tweetPerTopicCount[z]];
			perTweetPerplexity[z] = new double[tweetPerTopicCount[z]];
			tweetPerTopicCount[z] = 0;
		}
		String[] backgroundTweetID = new String[tweetBackgroundTopicCount];
		double[] backgroundTweetPerplexity = new double[tweetBackgroundTopicCount];
		tweetBackgroundTopicCount = 0;

		for (int u = 0; u < users.length; u++) {
			for (int t = 0; t < users[u].tweets.length; t++) {
				if (users[u].tweets[t].batch == testBatch)
					continue;
				int z = users[u].tweets[t].inferedTopic;
				if (z >= 0) {
					tweetID[z][tweetPerTopicCount[z]] = users[u].tweets[t].tweetID;
					perTweetPerplexity[z][tweetPerTopicCount[z]] = users[u].tweets[t].inferedLikelihood
							/ users[u].tweets[t].words.length;
					tweetPerTopicCount[z]++;
				} else {
					backgroundTweetID[tweetBackgroundTopicCount] = users[u].tweets[t].tweetID;
					backgroundTweetPerplexity[tweetBackgroundTopicCount] = users[u].tweets[t].inferedLikelihood;
					tweetBackgroundTopicCount++;
				}

			}
		}

		try {
			String fileName = outputPath + "/tweetTopicTopTweets.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			RankingTool rankTool = new RankingTool();
			WeightedElement[] topTweets = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(z + "\n");
				topTweets = rankTool.getTopKbyWeight(tweetID[z],
						perTweetPerplexity[z], k);
				for (int j = 0; j < k; j++)
					bw.write("," + topTweets[j].name + ","
							+ topTweets[j].weight + "\n");
			}
			bw.write("background\n");
			topTweets = rankTool.getTopKbyWeight(backgroundTweetID,
					backgroundTweetPerplexity, 2 * k);
			for (int j = 0; j < 2 * k; j++)
				bw.write("," + topTweets[j].name + "," + topTweets[j].weight
						+ "\n");

			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out tweet topic top tweets to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputUserTopicDistribution() {
		try {
			String fileName = outputPath + "/userTopics.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			for (int u = 0; u < users.length; u++) {
				bw.write("" + users[u].userID);
				for (int z = 0; z < nTopics; z++)
					bw.write("," + users[u].topicDistribution[z]);
				bw.write("\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out user topic distributions to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputLikelihoodPerplexity() {
		try {
			String fileName = outputPath + "/likelihood-perplexity.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));
			bw.write("tweetLogLikelihood,tweetLogPerplexity\n");
			bw.write("" + tweetLogLikelidhood + "," + tweetLogPerplexity);
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweets to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputInferedTopic() {
		try {
			SystemTool.createFolder(outputPath, "inferedTopicCoin");
			for (int u = 0; u < users.length; u++) {
				String fileName = outputPath + SystemTool.pathSeparator
						+ "inferedTopicCoin" + SystemTool.pathSeparator
						+ users[u].userID + ".txt";

				File file = new File(fileName);
				if (!file.exists()) {
					file.createNewFile();
				}
				BufferedWriter bw = new BufferedWriter(new FileWriter(
						file.getAbsoluteFile()));
				bw.write("tweets\n");
				for (int t = 0; t < users[u].tweets.length; t++)
					bw.write(users[u].tweets[t].tweetID + "\t"
							+ users[u].tweets[t].inferedTopic + "\n");
				bw.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTweetTopKTopics(int k) {
		try {
			String fileName = outputPath + SystemTool.pathSeparator
					+ "tweetTopKTopics.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(
					file.getAbsoluteFile()));

			String[] topics = new String[nTopics];
			RankingTool rankTool = new RankingTool();

			for (int z = 0; z < nTopics; z++)
				topics[z] = z + "";
			for (int u = 0; u < users.length; u++) {
				for (int t = 0; t < users[u].tweets.length; t++) {
					double[] probs = new double[nTopics];
					double sumProbs = 0;
					for (int z = 0; z < nTopics; z++) {
						probs[z] = 1;
						for (int i = 0; i < users[u].tweets[t].words.length; i++) {
							int w = users[u].tweets[t].words[i];
							probs[z] *= tweetTopics[z][w];
						}
						probs[z] *= users[u].topicDistribution[z];
						sumProbs += probs[z];
					}
					for (int z = 0; z < nTopics; z++)
						probs[z] /= sumProbs;
					WeightedElement[] topTopics = rankTool.getTopKbyWeight(
							topics, probs, k);
					bw.write(users[u].tweets[t].tweetID);
					for (int i = 0; i < k; i++)
						bw.write("," + topTopics[i].name + ","
								+ topTopics[i].weight);
					bw.write("\n");
				}
			}
			bw.close();
		} catch (Exception e) {
			System.out
					.println("Error in writing out tweet top topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void outputAll() {
		outputTweetTopics();
		outputTweetTopicTopWords(20);
		outputTweetTopicTopTweets(200);
		// outputInferedTopic();
		outputUserTopicDistribution();
		outputLikelihoodPerplexity();
		// outputTweetTopKTopics(3);
		outputCoinBias();
	}
}
